<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.2.0 for Hugo"><meta name=author content="Natasha Jaques"><meta name=description content><link rel=alternate hreflang=en-us href=https://natashamjaques.github.io/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.08f2e04360a1c87f5ad39547c02bf219.css><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Natasha Jaques"><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/media/icon_hua64261c49917fe48d46061d344453b21_144873_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hua64261c49917fe48d46061d344453b21_144873_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://natashamjaques.github.io/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Natasha Jaques"><meta property="og:url" content="https://natashamjaques.github.io/"><meta property="og:title" content="Natasha Jaques"><meta property="og:description" content><meta property="og:image" content="https://natashamjaques.github.io/media/icon_hua64261c49917fe48d46061d344453b21_144873_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://natashamjaques.github.io/media/icon_hua64261c49917fe48d46061d344453b21_144873_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-12-31T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://natashamjaques.github.io/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://natashamjaques.github.io/"}</script><title>Natasha Jaques</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper><script src=/js/wowchemy-init.min.4be02a3b391999348b0c7478778a0e4b.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Natasha Jaques</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Natasha Jaques</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#awards data-target=#awards><span>Awards</span></a></li><li class=nav-item><a class=nav-link href=/#press data-target=#press><span>Press</span></a></li><li class=nav-item><a class=nav-link href=/#featured data-target=#featured><span>Featured</span></a></li><li class=nav-item><a class=nav-link href=/#projects data-target=#projects><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#tags data-target=#tags><span>Topics</span></a></li><li class=nav-item><a class=nav-link href=/#talks data-target=#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#communities data-target=#communities><span>Communities</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://twitter.com/natashajaques data-toggle=tooltip data-placement=bottom title=Twitter target=_blank rel=noopener aria-label=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" src=/author/natasha-jaques/avatar_hu535661bce2c742bf7a0f6a055ac0b6d1_1827718_270x270_fill_q75_lanczos_center.jpg alt="Natasha Jaques"><div class=portrait-title><h2>Natasha Jaques</h2><h3><a href=https://research.google/people/NatashaJaques/ target=_blank rel=noopener><span>Senior Research Scientist</span></a></h3><h3><span>Google Brain</span></h3><h3><a href=http://rail.eecs.berkeley.edu/people.html target=_blank rel=noopener><span>Visiting Postdoctoral Scholar</span></a></h3><h3><span>UC Berkeley</span></h3></div><ul class=network-icon aria-hidden=true><li><a href=mailto:natashamjaques@gmail.com aria-label=envelope><i class="fas fa-envelope big-icon"></i></a></li><li><a href=https://twitter.com/natashajaques target=_blank rel=noopener aria-label=twitter><i class="fab fa-twitter big-icon"></i></a></li><li><a href="https://scholar.google.com/citations?hl=en&user=8iCb2TwAAAAJ" target=_blank rel=noopener aria-label=graduation-cap><i class="fas fa-graduation-cap big-icon"></i></a></li><li><a href=https://github.com/natashamjaques target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li><li><a href=/uploads/cv_natasha_jaques.pdf aria-label=download><i class="fas fa-download big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><div class=article-style><p>Social learning helps humans and animals rapidly adapt to new circumstances, and drives the emergence of complex learned behaviors. My research is focused on <strong>Social Reinforcement Learning</strong>&mdash;developing algorithms that combine insights from social learning and multi-agent training to improve AI agents&rsquo; <a href=./publication/paired>learning</a>, <a href=./publication/learning-social-learning/>generalization</a>, <a href=./publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/>coordination</a>, and <a href=./publication/humancentric-dialog-training-via-offline-reinforcement-learning/>human-AI interaction</a>.</p><p>I currently hold a joint position as a <a href=https://research.google/people/NatashaJaques/ target=_blank rel=noopener>Senior Research Scientist at Google Brain</a> and <a href=http://rail.eecs.berkeley.edu/people.html target=_blank rel=noopener>Visiting Postdoctoral Scholar at UC Berkeley</a>. I received my <a href=publication/social-and-affective-machine-learning/>PhD from MIT</a>, where I worked on <a href=./tag/affective-computing>Affective Computing</a> and <a href=./tag/deep-learning>deep</a>/<a href=./tag/reinforcement-learning>reinforcement</a>/<a href=./tag/machine-learning>machine learning</a>. For a brief overview of my thesis, check out this <a href=https://www.rsipvision.com/ComputerVisionNews-2021November/42/ target=_blank rel=noopener>write-up in Computer Vision News</a>. I have interned at DeepMind, Google Brain, and worked as an OpenAI Scholars mentor. <strong>I am currently on the faculty job market.</strong></p><p><i class="fas fa-download pr-1 fa-fw"></i><a href=/uploads/cv_natasha_jaques.pdf target=_blank>Download my CV</a>.</p></div><div class=row><div class=col-md-5><div class=section-subheading>Interests</div><ul class="ul-interests mb-0"><li>Multi-Agent Learning and Coordination</li><li>Human-AI Interaction</li><li>Affective Computing</li><li>Reinforcement Learning</li><li>Machine Learning</li></ul></div><div class=col-md-7><div class=section-subheading>Education</div><ul class="ul-edu fa-ul mb-0"><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>PhD in the Media Lab, 2019</p><p class=institution>Massachusetts Institute of Technology</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>MSc in Computer Science, 2014</p><p class=institution>University of British Columbia</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>BSc in Computer Science, 2012</p><p class=institution>University of Regina</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>BA in Psychology, 2012</p><p class=institution>University of Regina</p></div></li></ul></div></div></div></div></div></section><section id=awards class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Selected Awards</h1></div><div class="col-12 col-lg-8"><ul><li>2021 <a href="https://www.media.mit.edu/posts/natasha-jaques-best-phd-award/?fbclid=IwAR0dT73P4HeNolVWSFRwYAxrKaPl3pDoDw3CItIUE-5rlB5437y0C7-fYvI" target=_blank rel=noopener>Outstanding PhD Dissertation</a> from the international <a href=https://aaac.world/ target=_blank rel=noopener>Association for the Advancement of Affective Computing</a></li><li>2021 <a href=./publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/>Best of Collection</a> in the journal IEEE Transactions on Affective Computing (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5165369" target=_blank rel=noopener>impact factor: 10.5</a>)</li><li>2021 <a href=https://c3dti.ai/c3-announces-energy-climate-awards/ target=_blank rel=noopener>C3.ai Digital Transformation Institute AI for Energy and Climate Security Awards</a> funded our grant proposal, &ldquo;Offline Reinforcement Learning for Energy-Efficient Power Grids&rdquo;</li><li>2020 <a href=./publication/learning-social-learning>Best Paper</a> at the NeurIPS Workshop on Cooperative AI</li><li>2019 <a href=./publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/>Best Paper Honourable Mention</a> at the International Conference on Machine Learning (ICML) 2019</li><li>2019 <a href=https://www.eecs.mit.edu/news-events/announcements/mits-rising-stars-eecs-2019 target=_blank rel=noopener>Rising Stars in EECS</a> <a href=https://publish.illinois.edu/rising-stars/ target=_blank rel=noopener>Pitch Competition Winner</a></li><li>2019 <a href=./publication/hierarchical-reinforcement-learning-for-opendomain-dialog/>Best Paper Nominee</a> at the NeurIPS Workshop on Conversational AI</li><li>2017 <a href=https://campioncollege.ca/resources/natasha-jaques/ target=_blank rel=noopener>Centennial Alumni of Distinction</a> at Campion College</li><li>2016 <a href=./publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/>Best Paper</a> at the NeurIPS Workshop on ML for Healthcare</li><li>2016 <a href=publication/interactive-musical-improvisation-with-magenta/>Best Demo</a> at Neural Information Processing Systems (NeurIPS) 2016</li></ul></div></div></div></section><section id=press class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Selected Press</h1></div><div class="col-12 col-lg-8"><ul><li><a href=https://www.degreesmagazine.ca/the-skys-the-limit/2021/11/19/ target=_blank rel=noopener>Degrees Magazine</a>. Cataldo, S. (2021, November 19). <em>The sky&rsquo;s the limit.</em></li><li><a href=https://www.sciencemag.org/news/2021/01/who-needs-teacher-artificial-intelligence-designs-lesson-plans-itself target=_blank rel=noopener>Science</a>. Hutson, M. (2021, January 19). <em>Who needs a teacher? Artificial intelligence designs lesson plans for itself.</em></li><li><a href=https://spectrum.ieee.org/tech-talk/computing/software/deepmind-teaches-ai-teamwork target=_blank rel=noopener>IEEE Spectrum</a>. Hutson, M. (2019, June 17). <em>DeepMind Teaches AI Teamwork.</em></li><li><a href=https://www.technologyreview.com/s/603003/ai-songsmith-cranks-out-surprisingly-catchy-tunes/ target=_blank rel=noopener>MIT Technology Review</a>. Hao, K. (2019, June 20). <em>Here are 10 ways AI could help
fight climate change.</em></li><li><a href=https://www.nationalgeographic.com/environment/2019/07/artificial-intelligence-climate-change/ target=_blank rel=noopener>National Geographic</a>. Snow, J. (2019, July 18). <em>How artificial intelligence can tackle climate change.</em></li><li><a href=https://qz.com/1209466/google-is-building-ai-to-make-humans-smile/ target=_blank rel=noopener>Quartz</a>. Gershgorn, D. (2018, February 16). <em>Google is building AI to make humans smile.</em></li><li><a href=http://www.bostonmagazine.com/news/blog/2015/01/05/smiletracker-captures-photos-internet/ target=_blank rel=noopener>Boston Magazine</a>. Annear, S. (2015, January 5). <em>Website tracks your happiness to remind
you life’s not so bad.</em></li><li><a href=https://www.cbc.ca/news/canada/saskatchewan/regina-woman-develops-smile-app-at-mit-1.2886943 target=_blank rel=noopener>CBC radio</a>. Brace, S. (2015, January 5). <em>Regina woman develops smile app at MIT.</em></li></ul></div></div></div></section><section id=featured class="home-section wg-featured"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Featured Publications</h1></div><div class="col-12 col-lg-8"><div class=card-simple><div class=article-metadata><div><span>Michael Dennis</span><span>*</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>Eugene Vinitsky</span>, <span>Alexandre Bayen</span>, <span>Stuart Russell</span>, <span>Andrew Critch</span>, <span>Sergey Levine</span></div><span class=article-date>2020</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS)</em> <strong>Oral (top 1% of submissions)</strong></span></div><a href=/publication/paired/><img src=/publication/paired/featured_hu0186d5a70d0f2d327cff176d5e7f9ddc_218764_808x455_fit_q90_lanczos_3.png class=article-banner alt="Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design" loading=lazy></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/paired/>Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design</a></div><a href=/publication/paired/ class=summary-link><div class=article-style><p>PAIRED trains an agent to generate environments that maximize regret between a pair of learning agents. This creates feasible yet challenging environments, which exploit weaknesses in the agents to make them more robust. PAIRED significantly improves generalization to novel tasks.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2012.02096 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/paired/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/google-research/google-research/tree/master/social_rl/adversarial_env target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://bit.ly/paired_poster target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/1uQDmktf0PWNCFlliWLfPx-j4BtuhWNypONuYaQzzTXo/edit?usp=sharing" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.youtube.com/channel/UCI6dkF8eNrCz6XiBJlV9fmw/videos target=_blank rel=noopener>Videos</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://neurips.cc/virtual/2020/public/poster_985e9a46e10005356bbaf194249f6856.html target=_blank rel=noopener>NeurIPS Oral</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.sciencemag.org/news/2021/01/who-needs-teacher-artificial-intelligence-designs-lesson-plans-itself target=_blank rel=noopener>Science article</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ai.googleblog.com/2021/03/paired-new-multi-agent-approach-for.html target=_blank rel=noopener>Google AI Blog</a></div></div><div class=card-simple><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>J. H. Shen</span><span>*</span>, <span>A. Ghandeharioun</span>, <span>C. Ferguson</span>, <span>A. Lapedriza</span>, <span>N. Jones</span>, <span>S. Gu</span>, <span>R. Picard</span></div><span class=article-date>2020</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Empirical Methods in Natural Language Processing (EMNLP)</em></span></div><a href=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/><img src=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/featured_hud9f1a8371ef1afad2d7e710a56a9cc97_688018_808x455_fit_q90_lanczos_3.png class=article-banner alt="Human-Centric Dialog Training via Offline Reinforcement Learning" loading=lazy></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/>Human-Centric Dialog Training via Offline Reinforcement Learning</a></div><a href=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/ class=summary-link><div class=article-style><p>We train dialog models with interactive data from conversations with real humans, using a novel Offline RL technique based on KL-control. Rather than rely on manual ratings, we learn from implicit signals like sentiment, and show that this results in better performance.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2010.05848.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/natashamjaques/neural_chat target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/neural_chat/datasets/reddit_casual_preprocessed.tar.gz target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/1xd1ce8qiVlIorJoXylEWMLwLlMiqyLyAUMe1qhcVKO8/edit?usp=sharing" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://virtual.2020.emnlp.org/paper_main.2410.html target=_blank rel=noopener>EMNLP Talk</a></div></div><div class=card-simple><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>A. Lazaridou</span>, <span>E. Hughes</span>, <span>C. Gulcehre</span>, <span>P. A. Ortega</span>, <span>D. J. Strouse</span>, <span>J.Z. Leibo</span>, <span>N. de Freitas</span></div><span class=article-date>2019</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference on Machine Learning (ICML)</em> <strong>Best Paper Honourable Mention (top 0.26% of submissions)</strong></span></div><a href=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/><img src=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/featured_huec237f5424ca94f9712aa265373e0f2d_109370_808x455_fit_q90_lanczos_3.png class=article-banner alt="Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning" loading=lazy></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/>Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning</a></div><a href=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/ class=summary-link><div class=article-style><p>Social influence is a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning, through rewarding agents for having causal influence over other agents&rsquo; actions, thus increasing mutual information between agents&rsquo; actions. Optimizing for influence leads to agents learning emergent communication protocols. Unlike prior work, influence can be computed in a fully decentralized manner.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/1810.08647.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1itGMdTbCIkmK8uetNuW91RflWMeAs1q6/view?usp=sharing" target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/1g-H9WAXWf-VL_yNhW3-b260Kh5w-MPFw00LTU9mk3Q8/edit?usp=sharing" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.youtube.com/channel/UCNzeAAPyZaX4EDr720q5msg target=_blank rel=noopener>Videos</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.facebook.com/watch/live/?v=355035025132741&ref=watch_permalink" target=_blank rel=noopener>ICML talk</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://spectrum.ieee.org/tech-talk/computing/software/deepmind-teaches-ai-teamwork target=_blank rel=noopener>IEEE Spectrum article</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://medium.com/syncedreview/icml-2019-google-eth-zurich-mpi-is-cambridge-prowler-io-share-best-paper-honours-4aeabd5c9fc8 target=_blank rel=noopener>ICML 2019 Best Papers</a></div></div><div class=card-simple><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>S. Taylor</span><span>*</span>, <span>E. Nosakhare</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2017</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>IEEE Transactions on Affective Computing (TAFFC)</em> <strong>Best Paper</strong>; <em>NeurIPS Machine Learning for Healthcare (ML4HC) Workshop</em> <strong>Best Paper</strong></span></div><a href=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/><img src=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/featured_huc186f698c5fa4c42a5a6008bd314e0ac_60822_808x455_fit_q90_lanczos_3.png class=article-banner alt="Personalized Multitask Learning for Predicting Tomorrow's Mood, Stress, and Health" loading=lazy></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/>Personalized Multitask Learning for Predicting Tomorrow's Mood, Stress, and Health</a></div><a href=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/ class=summary-link><div class=article-style><p>Traditional, one-size-fits-all machine learning models fail to account for individual differences in predicting wellbeing outcomes like stress, mood, and health. Instead, we personalize models to the individual using multi-task learning (MTL), employing hierarchical Bayes, kernel-based and deep neural network MTL models to improve prediction accuracy by 13-23%.</p></div></a><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mitmedialab/PersonalizedMultitaskLearning target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=gFXVGGzxQvU" target=_blank rel=noopener>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://pdfs.semanticscholar.org/b228/7a406985980515d5cc63e9b37fb17c5186f8.pdf target=_blank rel=noopener>ML4HC Best Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/pdfs/17.TaylorJaques-PredictingTomorrowsMoods.pdf target=_blank rel=noopener>TAFFC Journal Best Paper</a></div></div></div></div></div></section><section id=projects class="home-section wg-portfolio"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Publications</h1></div><div class=col-12><p>To find relevant content, try <a href=./publication/>searching publications</a>, filtering using the buttons below, or exploring <a href=#tags>popular topics</a>. A * denotes equal contribution.</p><span class="d-none default-project-filter">*</span><div class=project-toolbar><div class=project-filters><div class=btn-toolbar><div class="btn-group flex-wrap"><a href=# data-filter=* class="btn btn-primary btn-lg active">All</a>
<a href=# data-filter=.js-id-Affective-Computing class="btn btn-primary btn-lg">Affective Computing</a>
<a href=# data-filter=.js-id-Social-Learning class="btn btn-primary btn-lg">Social Learning</a>
<a href=# data-filter=.js-id-Multi-Agent class="btn btn-primary btn-lg">Multi-Agent</a>
<a href=# data-filter=.js-id-Emergent-Complexity class="btn btn-primary btn-lg">Emergent Complexity</a>
<a href=# data-filter=.js-id-Cooperation class="btn btn-primary btn-lg">Cooperation</a>
<a href=# data-filter=.js-id-Communication-and-Language class="btn btn-primary btn-lg">Communication and Language</a>
<a href=# data-filter=.js-id-Human-AI-Interaction class="btn btn-primary btn-lg">Human-AI Interaction</a>
<a href=# data-filter=.js-id-Generalization class="btn btn-primary btn-lg">Generalization</a></div></div></div></div><div class="isotope projects-container row js-layout-row"><div class="col-12 isotope-item"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/conceptbased-understanding-of-emergent-multiagent-behavior/>Concept-based Understanding of Emergent Multi-Agent Behavior</a></div><a href=/publication/conceptbased-understanding-of-emergent-multiagent-behavior/ class=summary-link><div class=article-style>Interpreting whether multi-agent reinforcement learning (MARL) agents have successfully learned to coordinate with each other, versus finding some other way to exploit the reward function, is a longstanding problem. We develop a novel interpretability method for MARL based on concept bottlenecks, which enables detecting which agents are truly coordinating, which environments require coordination, and identifying lazy agents.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>N. Grupen</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>B. Kim</span>, <span>S. Omidshafiei</span></div><span class=article-date>2022</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Preprint</em></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/conceptbased-understanding-of-emergent-multiagent-behavior/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/conceptbased-understanding-of-emergent-multiagent-behavior/><img src=/publication/conceptbased-understanding-of-emergent-multiagent-behavior/compact_hu705aa51fbfbe67be943ca8eef05b05bf_386948_300x0_resize_lanczos_3.png alt="Concept-based Understanding of Emergent Multi-Agent Behavior" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/moral-foundations-of-large-language-models/>Moral Foundations of Large Language Models</a></div><a href=/publication/moral-foundations-of-large-language-models/ class=summary-link><div class=article-style>Moral Foundations theory decomposes human moral reasoning into five factors, which vary reliably across different human populations and political affiliations. We use moral foundations to analyze large language models like GPT-3 to determine what, if any, consistent moral values it brings to conversations, whether these can be deliberately manipulated, and whether holding a particular moral stance affects downstream tasks.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>M. Abdulhai</span>, <span>C. Crepy</span>, <span>D. Valter</span>, <span>J. Canny</span>, <span>S. Levine</span>, <span class=author-highlighted>Natasha Jaques</span></div><span class=article-date>2022</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Preprint</em></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/moral-foundations-of-large-language-models/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/moral-foundations-of-large-language-models/><img src=/publication/moral-foundations-of-large-language-models/compact_hue9a01ee0f7f717c448c0d9c0768e8c57_264340_300x0_resize_lanczos_3.png alt="Moral Foundations of Large Language Models" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/in-the-zone-measuring-difficulty-and--progression-in-curriculum-genera/>In the ZONE: Measuring difficulty and progression in curriculum generation</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span>R. E. Wang</span>, <span>J. Mu</span>, <span>D. Arumugam</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>N. Goodman</span></div><span class=article-date>2022</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Preprint</em></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/in-the-zone-measuring-difficulty-and--progression-in-curriculum-genera/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"></div></div></div></div><div class="col-12 isotope-item"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/multiagent-reinforcement-learning-for-hardware-architecture-search-a-c/>Multi-Agent Reinforcement Learning for Hardware Architecture Search: A Case Study on Domain-Specific DRAM Memory Controller Design</a></div><a href=/publication/multiagent-reinforcement-learning-for-hardware-architecture-search-a-c/ class=summary-link><div class=article-style>Reinforement Learning can potentially be a powerful tool for solving complex combinatorial optimization problems, such as microprocessor desgin. However, past work in this space has only investigated single-agent RL approaches. Here, we show that a multi-agent RL approach outperforms these baselines, since the problem can easily be decomposed into designing independent sub-systems.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>S. Krishnan</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>S. Omidshafiei</span>, <span>D. Zhang</span>, <span>I. Gur</span>, <span>V. J. Reddi</span>, <span>S. Faust</span></div><span class=article-date>2022</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Preprint</em></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/multiagent-reinforcement-learning-for-hardware-architecture-search-a-c/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/multiagent-reinforcement-learning-for-hardware-architecture-search-a-c/><img src=/publication/multiagent-reinforcement-learning-for-hardware-architecture-search-a-c/compact_hu3926ae8454c436041a054105300217fb_190455_300x0_resize_lanczos_3.png alt="Multi-Agent Reinforcement Learning for Hardware Architecture Search: A Case Study on Domain-Specific DRAM Memory Controller Design" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/basis-for-intentions-efficient-inverse-reinforcement-learning-using-pa/>Basis for Intentions: Efficient Inverse Reinforcement Learning using Past Experience</a></div><a href=/publication/basis-for-intentions-efficient-inverse-reinforcement-learning-using-pa/ class=summary-link><div class=article-style>Using inverse reinforcement learning to infer human preferences is challenging, because it is an underspecified problem. We use multi-task RL pre-training and successor features to learn a strong prior over the space of reasonable goals in an environment&mdash;which we call a <em>basis</em>&mdash;that enables rapidly inferring an expert&rsquo;s reward function in only 100 samples.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>M. Abdulhai</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>S. Levine</span></div><span class=article-date>2022</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Preprint</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2208.04919 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/basis-for-intentions-efficient-inverse-reinforcement-learning-using-pa/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/abdulhaim/basis-irl target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://sites.google.com/view/basis-irl target=_blank rel=noopener>Project</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/basis-for-intentions-efficient-inverse-reinforcement-learning-using-pa/><img src=/publication/basis-for-intentions-efficient-inverse-reinforcement-learning-using-pa/compact_hu2c428969caf457db5e6fa3eae4c00a45_50663_300x0_resize_lanczos_3.png alt="Basis for Intentions: Efficient Inverse Reinforcement Learning using Past Experience" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Human-AI-Interaction js-id-Communication-and-Language js-id-Deep-Learning js-id-Machine-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/less-is-more-generating-grounded-navigation-instructions-from-landmark/>Less is More: Generating Grounded Navigation Instructions from Landmarks</a></div><a href=/publication/less-is-more-generating-grounded-navigation-instructions-from-landmark/ class=summary-link><div class=article-style>We study the automatic generation of natural language navigation instructions in visually realistic indoor environments. Existing generators suffer from poor visual grounding, skip steps, and hallucinate objects. We address this using a large language model which incorporates visual landmark detection.. The model dramatically increases the quality of generated instructions, such that humans can follow them with a 71% success rate (SR); just shy of the 75% SR of real human instructions.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>S. Wang</span>, <span>C. Montgomery</span>, <span>J. Orbay</span>, <span>V. Birodkar</span>, <span>A. Faust</span>, <span>I. Gur</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>A. Waters</span>, <span>J. Baldridge</span>, <span>P. Anderson</span></div><span class=article-date>2022</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Computer Vision and Pattern Recognition (CVPR)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2111.12872 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/less-is-more-generating-grounded-navigation-instructions-from-landmark/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/google-research-datasets/RxR/tree/main/marky-mT5 target=_blank rel=noopener>Dataset</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/less-is-more-generating-grounded-navigation-instructions-from-landmark/><img src=/publication/less-is-more-generating-grounded-navigation-instructions-from-landmark/compact_huda987d26fed255ff36440dafe01526f5_3231341_300x0_resize_lanczos_3.png alt="Less is More: Generating Grounded Navigation Instructions from Landmarks" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/wearables-an-r-package-with-accompanying-shiny-application-for-signal-/>Wearables: an R package with accompanying Shiny application for signal analysis of a wearable device targeted at clinicians and researchers</a></div><a href=/publication/wearables-an-r-package-with-accompanying-shiny-application-for-signal-/ class=summary-link><div class=article-style>Physiological signals like heart rate and skin conductance collected from wearable devices open up a range of interesting research for clinicians and psychologists, including studying physiological reactivity to daily events and stressors. We introduce a new R package and application for analyzing wearable physiological data which enables large scale processing, and ease of use in gaining insight into this data.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>P. de Looff</span>, <span>R. Duursma</span>, <span>Noordzij. Noordzi</span>, <span>S. Taylor</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>F. Scheepers</span>, <span>K. De Schepper</span>, <span>S. Koldijk</span></div><span class=article-date>2022</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Frontiers in behavioral neuroscience</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.frontiersin.org/articles/10.3389/fnbeh.2022.856544/pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wearables-an-r-package-with-accompanying-shiny-application-for-signal-/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/PCdLf/e4dashboard target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/PCdLf/wearables target=_blank rel=noopener>R Package</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/wearables-an-r-package-with-accompanying-shiny-application-for-signal-/><img src=/publication/wearables-an-r-package-with-accompanying-shiny-application-for-signal-/compact_hu730525697960ca5dba8d3815757c153d_1500009_300x0_resize_lanczos_3.png alt="Wearables: an R package with accompanying Shiny application for signal analysis of a wearable device targeted at clinicians and researchers" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Emergent-Complexity js-id-Multi-Agent js-id-Generalization js-id-Reinforcement-Learning js-id-Deep-Learning js-id-Web-Navigation"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/environment-generation-for-zero-shot-compositional-reinforcement-learn/>Environment Generation for Zero-Shot Compositional Reinforcement Learning</a></div><a href=/publication/environment-generation-for-zero-shot-compositional-reinforcement-learn/ class=summary-link><div class=article-style>We analyze and improve upon PAIRED in the case of learning to generate challenging compositional tasks. We apply our improved algorithm to the complex task of training RL agents to navigate websites, and find that it is able to generating a challenging curriculum of novel sites. We achieve a 4x improvement over the strongest web navigation baselines, and deploy our model to navigate real-world websites..</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>I. Gur</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>K. Malta</span>, <span>M. Tiwari</span>, <span>H. Lee</span>, <span>A. Faust</span></div><span class=article-date>2021</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://papers.nips.cc/paper/2021/file/218344619d8fb95d504ccfa11804073f-Paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/environment-generation-for-zero-shot-compositional-reinforcement-learn/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/environment-generation-for-zero-shot-compositional-reinforcement-learn/><img src=/publication/environment-generation-for-zero-shot-compositional-reinforcement-learn/compact_hu7dc49dc34a48ac2cf1cbe1a059b778c0_44325_300x0_resize_lanczos_3.png alt="Environment Generation for Zero-Shot Compositional Reinforcement Learning" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Social-Learning js-id-Multi-Agent js-id-Generalization js-id-Reinforcement-Learning js-id-Successor-Features js-id-Deep-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/>PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning</a></div><a href=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/ class=summary-link><div class=article-style>PsiPhi-Learning learns successor representations for the policies of other agents and the ego agent, using a shared underlying state representation. Learning from other agents helps the agent take better actions at inference time, and learning from RL experience improves modeling of other agents.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Filos</span>, <span>C. Lyle</span>, <span>Y. Gal</span>, <span>S. Levine</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>G. Farquhar</span><span>*</span></div><span class=article-date>2021</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference on Machine Learning (ICML)</em> <strong>Oral (top 3% of submissions)</strong></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2102.12560.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://sites.google.com/view/psiphi-learning target=_blank rel=noopener>Project</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=8aHElAZB5uo" target=_blank rel=noopener>ICML talk</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/><img src=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/compact_hu7430deb91a31ed263679bc79272f4499_35810_300x0_resize_lanczos_3.png alt="PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Social-Learning js-id-Multi-Agent js-id-Generalization js-id-Reinforcement-Learning js-id-Deep-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/learning-social-learning/>Emergent Social Learning via Multi-agent Reinforcement Learning</a></div><a href=/publication/learning-social-learning/ class=summary-link><div class=article-style>Model-free RL agents fail to learn from experts present in multi-agent environments. By adding a model-based auxiliary loss, we induce social learning, which allows agents to learn how to learn from experts. When deployed to novel environments with new experts, they use social learning to determine how to solve the task, and generalize better than agents trained alone with RL or imitation learning.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>Kamal Ndousse</span>, <span>Douglas Eck</span>, <span>Sergey Levine</span>, <span class=author-highlighted>Natasha Jaques</span></div><span class=article-date>2021</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference on Machine Learning (ICML);</em> <em>NeurIPS Cooperative AI Workshop</em> <strong>Best Paper</strong></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2010.00581 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/learning-social-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/kandouss/marlgrid target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1cIggGDJeFrKc3GrVELNgGxqj4UuMd-GE/view?usp=sharing" target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/1iQcqofu-_MLi9v21KoZR7Hb-PAsY15y29-4JYnMXoLc/edit?usp=sharing" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://slideslive.com/38938232/learning-social-learning?ref=account-folder-62099-folders" target=_blank rel=noopener>Cooperative AI talk</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=UctYVUn01ZA" target=_blank rel=noopener>ICML talk</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/learning-social-learning/><img src=/publication/learning-social-learning/compact_huf868c3ebd1f8795d3075bfbec778af36_146141_300x0_resize_lanczos_3.png alt="Emergent Social Learning via Multi-agent Reinforcement Learning" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Emergent-Complexity js-id-Multi-Agent js-id-Intrinsic-Motivation js-id-Reinforcement-Learning js-id-Deep-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/explore-and-control-with-adversarial-surprise/>Explore and Control with Adversarial Surprise</a></div><a href=/publication/explore-and-control-with-adversarial-surprise/ class=summary-link><div class=article-style>Adversarial Surprise creates a competitive game between an Expore policy and a Control policy, which fight to maximize and minimize the amount of entropy an RL agent experiences. We show both theoretically and empirically that this technique fully explores the state space of partially-observed, stochastic environments.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Fickinger</span><span>*</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>S. Parajuli</span>, <span>M. Chang</span>, <span>N. Rhinehart</span>, <span>G. Berseth</span>, <span>S. Russell</span>, <span>S. Levine</span></div><span class=article-date>2021</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>ICML Unsupervised Reinforcement Learning workshop</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2107.07394 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/explore-and-control-with-adversarial-surprise/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/ArnaudFickinger/adversarial-surprise target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://sites.google.com/view/adversarial-surprise/home target=_blank rel=noopener>Project</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/explore-and-control-with-adversarial-surprise/><img src=/publication/explore-and-control-with-adversarial-surprise/compact_huc6c3ce620201bb8c7060779d4ed52a28_144473_300x0_resize_lanczos_3.png alt="Explore and Control with Adversarial Surprise" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Multi-Agent js-id-Cooperation js-id-Intrinsic-Motivation js-id-Social-Learning js-id-Reinforcement-Learning js-id-Deep-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/joint-attention-for-multiagent-coordination-and-social-learning/>Joint Attention for Multi-Agent Coordination and Social Learning</a></div><a href=/publication/joint-attention-for-multiagent-coordination-and-social-learning/ class=summary-link><div class=article-style>Joint attention is a critical component of human social cognition. In this paper, we ask whether a mechanism based on shared visual attention can be useful for improving multi-agent coordination and social learning.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>D. Lee</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>J. Kew</span>, <span>D. Eck</span>, <span>D. Schuurmans</span>, <span>A. Faust</span></div><span class=article-date>2021</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>ICRA Social Intelligence Workshop</em> <strong>Spotlight talk</strong></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2104.07750 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/joint-attention-for-multiagent-coordination-and-social-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/google-research/google-research/tree/master/social_rl/multiagent_tfagents/joint_attention target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1Dq0kQUgSAzyj601n8CCC1u_8ecrypLPv/view?usp=sharing" target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=v3PXnGQ_tYI" target=_blank rel=noopener>Video</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/joint-attention-for-multiagent-coordination-and-social-learning/><img src=/publication/joint-attention-for-multiagent-coordination-and-social-learning/compact_hue339bf41b041e14e5bfbb28aaed1fe45_240472_300x0_resize_lanczos_3.png alt="Joint Attention for Multi-Agent Coordination and Social Learning" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Emergent-Complexity js-id-Multi-Agent js-id-Generalization js-id-Deep-Learning js-id-Reinforcement-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/paired/>Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design</a></div><a href=/publication/paired/ class=summary-link><div class=article-style>PAIRED trains an agent to generate environments that maximize regret between a pair of learning agents. This creates feasible yet challenging environments, which exploit weaknesses in the agents to make them more robust. PAIRED significantly improves generalization to novel tasks.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>Michael Dennis</span><span>*</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>Eugene Vinitsky</span>, <span>Alexandre Bayen</span>, <span>Stuart Russell</span>, <span>Andrew Critch</span>, <span>Sergey Levine</span></div><span class=article-date>2020</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS)</em> <strong>Oral (top 1% of submissions)</strong></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2012.02096 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/paired/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/google-research/google-research/tree/master/social_rl/adversarial_env target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://bit.ly/paired_poster target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/1uQDmktf0PWNCFlliWLfPx-j4BtuhWNypONuYaQzzTXo/edit?usp=sharing" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.youtube.com/channel/UCI6dkF8eNrCz6XiBJlV9fmw/videos target=_blank rel=noopener>Videos</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://neurips.cc/virtual/2020/public/poster_985e9a46e10005356bbaf194249f6856.html target=_blank rel=noopener>NeurIPS Oral</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.sciencemag.org/news/2021/01/who-needs-teacher-artificial-intelligence-designs-lesson-plans-itself target=_blank rel=noopener>Science article</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ai.googleblog.com/2021/03/paired-new-multi-agent-approach-for.html target=_blank rel=noopener>Google AI Blog</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/paired/><img src=/publication/paired/compact_huc914225ca940260e2866390a3c6be412_103258_300x0_resize_lanczos_3.png alt="Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Human-AI-Interaction js-id-Social-Learning js-id-Affective-Computing js-id-Offline-RL js-id-Reinforcement-Learning js-id-Communication-and-Language js-id-Deep-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/>Human-Centric Dialog Training via Offline Reinforcement Learning</a></div><a href=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/ class=summary-link><div class=article-style>We train dialog models with interactive data from conversations with real humans, using a novel Offline RL technique based on KL-control. Rather than rely on manual ratings, we learn from implicit signals like sentiment, and show that this results in better performance.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>J. H. Shen</span><span>*</span>, <span>A. Ghandeharioun</span>, <span>C. Ferguson</span>, <span>A. Lapedriza</span>, <span>N. Jones</span>, <span>S. Gu</span>, <span>R. Picard</span></div><span class=article-date>2020</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Empirical Methods in Natural Language Processing (EMNLP)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2010.05848.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/natashamjaques/neural_chat target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/neural_chat/datasets/reddit_casual_preprocessed.tar.gz target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/1xd1ce8qiVlIorJoXylEWMLwLlMiqyLyAUMe1qhcVKO8/edit?usp=sharing" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://virtual.2020.emnlp.org/paper_main.2410.html target=_blank rel=noopener>EMNLP Talk</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/><img src=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/compact_hucee76efa9adcfddb2507c2302e8a6874_104998_300x0_resize_lanczos_3.png alt="Human-Centric Dialog Training via Offline Reinforcement Learning" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Communication-and-Language js-id-Affective-Computing js-id-Machine-Learning js-id-Human-AI-Interaction js-id-Deep-Learning js-id-Machine-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/approximating-interactive-human-evaluation-with-selfplay-for-opendomai/>Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems</a></div><a href=/publication/approximating-interactive-human-evaluation-with-selfplay-for-opendomai/ class=summary-link><div class=article-style>Existing metrics for automatically evaluating dialog models correlate poorly with human judgements, and are evaluated on static conversation snippets. Instead, we deploy bots to interact live with humans, then approximate human ratings with state-of-the-art accuracy using conversations generated with self-play.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Ghandeharioun</span><span>*</span>, <span>J. H. Shen</span><span>*</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>C. Ferguson</span>, <span>N. Jones</span>, <span>A. Lapedriza</span>, <span>R. Picard</span></div><span class=article-date>2019</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1906.09308 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/approximating-interactive-human-evaluation-with-selfplay-for-opendomai/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/natashamjaques/neural_chat target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/neural_chat/datasets/reddit_casual_preprocessed.tar.gz target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1Wr6n5KB9GDWUQun5TWrWjH7I1wrtXY28/view?usp=sharing" target=_blank rel=noopener>Poster</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/approximating-interactive-human-evaluation-with-selfplay-for-opendomai/><img src=/publication/approximating-interactive-human-evaluation-with-selfplay-for-opendomai/compact_hu66e60844ef6f4d3eeada98744e8de13e_292165_300x0_resize_lanczos_3.png alt="Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Machine-Learning js-id-Healthcare js-id-Deep-Learning js-id-Wellbeing"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/automatic-triage-and-analysis-of-online-suicide-risk-with-document-emb/>Automatic Triage and Analysis of Online Suicide Risk with Document Embeddings and Latent Dirichlet Allocation</a></div><a href=/publication/automatic-triage-and-analysis-of-online-suicide-risk-with-document-emb/ class=summary-link><div class=article-style>To predict which users are at risk of suicide based on a small dataset of online posts, we leverage pre-trained sentence embeddings from large language models, and achieve high F1 scores (.83-.92). We further analyze users&rsquo; posts to determine which topics are most associated with suicidal users.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>N. Jones</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>P. Pataranutaporn</span>, <span>A. Ghandeharioun</span>, <span>R. Picard</span></div><span class=article-date>2019</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Affective Computing and Intelligence Interaction (ACII) workshop on Machine Learning for Mental Health</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1muoFj_BXJUZCRyjCLEX9DxKOz7b9nKtj/view?usp=sharing" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/automatic-triage-and-analysis-of-online-suicide-risk-with-document-emb/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/automatic-triage-and-analysis-of-online-suicide-risk-with-document-emb/><img src=/publication/automatic-triage-and-analysis-of-online-suicide-risk-with-document-emb/compact_huabc2c2a55c60ce3e36a4b80033463568_83028_300x0_resize_lanczos_3.png alt="Automatic Triage and Analysis of Online Suicide Risk with Document Embeddings and Latent Dirichlet Allocation" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Communication-and-Language js-id-Reinforcement-Learning js-id-Hierarchical-Reinforcement-Learning js-id-Deep-Learning js-id-Sequence-Modeling"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/hierarchical-reinforcement-learning-for-opendomain-dialog/>Hierarchical Reinforcement Learning for Open-Domain Dialog</a></div><a href=/publication/hierarchical-reinforcement-learning-for-opendomain-dialog/ class=summary-link><div class=article-style>For the first time, we use hierarchical reinforcement learning to train open-domain dialog models, enabling the optimization of long-term, conversational, rewards, including reducing the toxicity of generated language. Our approach provides significant improvements over state-of-the-art dialog models.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Saleh</span><span>*</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>A. Ghandeharioun</span>, <span>J. H. Shen</span>, <span>R. Picard</span></div><span class=article-date>2019</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Association for the Advancement of Artificial Intelligence (AAAI)</em> <strong>Oral (top 7.8% of submissions)</strong></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1909.07547 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/hierarchical-reinforcement-learning-for-opendomain-dialog/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/natashamjaques/neural_chat/tree/master/HierarchicalRL target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/neural_chat/datasets/reddit_casual_preprocessed.tar.gz target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://slideslive.com/38922318/contributed-talk-4-hierarchical-reinforcement-learning-for-opendomain-dialog target=_blank rel=noopener>Talk</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/hierarchical-reinforcement-learning-for-opendomain-dialog/><img src=/publication/hierarchical-reinforcement-learning-for-opendomain-dialog/compact_huab7f30131204beb0dd6375a7a0eb8990_154201_300x0_resize_lanczos_3.png alt="Hierarchical Reinforcement Learning for Open-Domain Dialog" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Human-AI-Interaction js-id-Multi-Agent js-id-Cooperation js-id-Communication-and-Language js-id-Generalization js-id-Social-Learning js-id-Deep-Learning js-id-Reinforcement-Learning js-id-Machine-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/social-and-affective-machine-learning/>Social and Affective Machine Learning</a></div><a href=/publication/social-and-affective-machine-learning/ class=summary-link><div class=article-style>My PhD Thesis spans both Social Reinforcement Learning and Affective Computing, investigating how affective and social intelligence can enhance machine learning algorithms, and how machine learning can enhance our ability to predict and understand human affective and social phenomena.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span></div><span class=article-date>2019</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Massachusetts Institute of Technology</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.media.mit.edu/publications/social-and-affective-machine-learning/ target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/social-and-affective-machine-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=a-oWa2CS8jg" target=_blank rel=noopener>Thesis Defense</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.rsipvision.com/ComputerVisionNews-2021November/42/ target=_blank rel=noopener>CV News write-up</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/social-and-affective-machine-learning/><img src=/publication/social-and-affective-machine-learning/compact_hu92d99a804c27a12d0bbd429fe5c42e10_180638_300x0_resize_lanczos_3.png alt="Social and Affective Machine Learning" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Cooperation js-id-Multi-Agent js-id-Communication-and-Language js-id-Intrinsic-Motivation js-id-Reinforcement-Learning js-id-Deep-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/>Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning</a></div><a href=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/ class=summary-link><div class=article-style>Social influence is a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning, through rewarding agents for having causal influence over other agents&rsquo; actions, thus increasing mutual information between agents&rsquo; actions. Optimizing for influence leads to agents learning emergent communication protocols. Unlike prior work, influence can be computed in a fully decentralized manner.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>A. Lazaridou</span>, <span>E. Hughes</span>, <span>C. Gulcehre</span>, <span>P. A. Ortega</span>, <span>D. J. Strouse</span>, <span>J.Z. Leibo</span>, <span>N. de Freitas</span></div><span class=article-date>2019</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference on Machine Learning (ICML)</em> <strong>Best Paper Honourable Mention (top 0.26% of submissions)</strong></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/1810.08647.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1itGMdTbCIkmK8uetNuW91RflWMeAs1q6/view?usp=sharing" target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/1g-H9WAXWf-VL_yNhW3-b260Kh5w-MPFw00LTU9mk3Q8/edit?usp=sharing" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.youtube.com/channel/UCNzeAAPyZaX4EDr720q5msg target=_blank rel=noopener>Videos</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.facebook.com/watch/live/?v=355035025132741&ref=watch_permalink" target=_blank rel=noopener>ICML talk</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://spectrum.ieee.org/tech-talk/computing/software/deepmind-teaches-ai-teamwork target=_blank rel=noopener>IEEE Spectrum article</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://medium.com/syncedreview/icml-2019-google-eth-zurich-mpi-is-cambridge-prowler-io-share-best-paper-honours-4aeabd5c9fc8 target=_blank rel=noopener>ICML 2019 Best Papers</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/><img src=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/compact_hue1151792439960bc84c366ff900f0c0a_61909_300x0_resize_lanczos_3.png alt="Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Climate-Change js-id-Machine-Learning js-id-Deep-Learning js-id-Reinforcement-Learning js-id-Social-Good"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/tackling-climate-change-with-machine-learning/>Tackling Climate Change with Machine Learning</a></div><a href=/publication/tackling-climate-change-with-machine-learning/ class=summary-link><div class=article-style>This paper comprehensively surveys the ways in which machine learning could be usefully deployed in the fight against climate change. From smart grids to disaster management, we identify high impact problems and outline how machine learning can be employed to address them.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>D. Rolnick</span>, <span>P. L. Donti</span>, <span>L. H. Kaack</span>, <span>K. Kochanski</span>, <span>A. Lacoste</span>, <span>K. Sankaran</span>, <span>A. S. Ross</span>, <span>N. Milojevic-Dupont</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>A. Waldman-Brown</span>, <span>A. Luccioni</span>, <span>T. Maharaj</span>, <span>E. D. Sherwin</span>, <span>S. K. Mukkavilli</span>, <span>K. P. Kording</span>, <span>C. Gomes</span>, <span>A. Y. Ng</span>, <span>D. Hassabis</span>, <span>J. C. Platt</span>, <span>F. Creutzig</span>, <span>J. Chayes</span>, <span>Y. Bengio</span></div><span class=article-date>2019</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>ACM Computing Surveys</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1906.05433 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/tackling-climate-change-with-machine-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.climatechange.ai/ target=_blank rel=noopener>CCAI Organization</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.technologyreview.com/2019/06/20/134864/ai-climate-change-machine-learning/ target=_blank rel=noopener>MIT Tech Review article</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.nationalgeographic.com/environment/article/artificial-intelligence-climate-change target=_blank rel=noopener>National Geographic article</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/tackling-climate-change-with-machine-learning/><img src=/publication/tackling-climate-change-with-machine-learning/compact_huef4affcf62397ca1d23e957050e42994_211889_300x0_resize_lanczos_3.png alt="Tackling Climate Change with Machine Learning" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Social-Learning js-id-Human-AI-Interaction js-id-Affective-Computing js-id-Generative-Models js-id-Deep-Learning js-id-Machine-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/learning-via-social-awareness-improving-a-deep-generative-sketching-mo/>Learning via Social Awareness: Improving a Deep Generative Sketching Model with Facial Feedback</a></div><a href=/publication/learning-via-social-awareness-improving-a-deep-generative-sketching-mo/ class=summary-link><div class=article-style>We show the outputs of a generative model of sketches to human observers and record their facial expressions. Using only a small number of facial expression samples, we are able to tune the model to produce drawings that are significantly better rated by humans.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>J. McCleary</span>, <span>J. Engel</span>, <span>D. Ha</span>, <span>F. Bertsch</span>, <span>D. Eck</span>, <span>R. Picard</span></div><span class=article-date>2018</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference on Learning Representations (ICLR) workshop</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/1802.04877.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/learning-via-social-awareness-improving-a-deep-generative-sketching-mo/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.slideshare.net/lubaelliott/natasha-jaques-learning-via-social-awareness-creative-ai-meetup target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://qz.com/1209466/google-is-building-ai-to-make-humans-smile/ target=_blank rel=noopener>Quartz article</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/learning-via-social-awareness-improving-a-deep-generative-sketching-mo/><img src=/publication/learning-via-social-awareness-improving-a-deep-generative-sketching-mo/compact_hu05080f9b36edf6ab84c6a6644d3e33d8_525540_300x0_resize_lanczos_3.png alt="Learning via Social Awareness: Improving a Deep Generative Sketching Model with Facial Feedback" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Sensors js-id-Physiology js-id-Machine-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/vomit-comet-physiology-autonomic-changes-in-novice-flyers/>Vomit Comet Physiology: Autonomic Changes in Novice Flyers</a></div><a href=/publication/vomit-comet-physiology-autonomic-changes-in-novice-flyers/ class=summary-link><div class=article-style>During a zero-gravity parabolic flight, we recorded participants&rsquo; heart rate, accelerometer, and skin conductance measurements as well as their self-report nausea, anxiety, and excitement. Statistical analysis revealed that skin conductance is predictive of nausea, while heart rate is predictive of anxiety and excitement.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>K. Johnson</span>, <span>S. Taylor</span>, <span>S. Fedor</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>W. Chen</span>, <span>R. Picard</span></div><span class=article-date>2018</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>IEEE Engineering in Medicine and Biology Society (EMBC)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://dspace.mit.edu/bitstream/handle/1721.1/123805/18.Johnson-etal_EMBC18_VomitComet.pdf?sequence=1&isAllowed=y" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/vomit-comet-physiology-autonomic-changes-in-novice-flyers/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/vomit-comet-physiology-autonomic-changes-in-novice-flyers/><img src=/publication/vomit-comet-physiology-autonomic-changes-in-novice-flyers/compact_hubd65089fb19795e91750ea4f9066e2e4_424657_300x0_resize_lanczos_3.png alt="Vomit Comet Physiology: Autonomic Changes in Novice Flyers" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Sensors js-id-Physiology js-id-Healthcare js-id-Machine-Learning js-id-Hierarchical-Bayes js-id-Wellbeing"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/importance-of-sleep-data-in-predicting-nextday-stress-happiness-and-he/>Importance of Sleep Data in Predicting Next-Day Stress, Happiness, and Health in College Students</a></div><a href=/publication/importance-of-sleep-data-in-predicting-nextday-stress-happiness-and-he/ class=summary-link><div class=article-style>We train personalized hierarchical Bayes models to predict individual&rsquo;s next-day stress, happiness, and health, and examine the effect of including features related to sleep in the model. Including sleep features significantly improves performance when predicting happiness.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>S. Taylor</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>Sano, A. E. Nosakhare</span>, <span>E. B. Klerman</span>, <span>R. Picard</span></div><span class=article-date>2017</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Journal of Sleep and Sleep Disorders Research (suppl_1)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/pdfs/17.Taylor-etal-MoodPrediction-SLEEP2017-Poster.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/importance-of-sleep-data-in-predicting-nextday-stress-happiness-and-he/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/importance-of-sleep-data-in-predicting-nextday-stress-happiness-and-he/><img src=/publication/importance-of-sleep-data-in-predicting-nextday-stress-happiness-and-he/compact_hu69dd9bb9b0627592c4cc65fd4599288a_569771_300x0_resize_lanczos_3.png alt="Importance of Sleep Data in Predicting Next-Day Stress, Happiness, and Health in College Students" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Generalization js-id-Healthcare js-id-Deep-Learning js-id-Machine-Learning js-id-Sensors"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/multimodal-autoencoder-a-deep-learning-approach-to-filling-in-missing-/>Multimodal Autoencoder: A Deep Learning Approach to Filling in Missing Sensor Data and Enabling Better Mood Prediction</a></div><a href=/publication/multimodal-autoencoder-a-deep-learning-approach-to-filling-in-missing-/ class=summary-link><div class=article-style>Predicting signals like stress and health depends on collecting noisy data from a number of modalities, e.g. smartphone data, or physiological data from a wrist-worn sensor. Our method can continue making accurate predictions even when a modality goes missing; for example, if the person forgets to wear their sensor.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>S. Taylor</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2017</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference on Affective Computing and Intelligent Interaction (ACII)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/pdfs/17.Jaques_autoencoder_ACII.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/multimodal-autoencoder-a-deep-learning-approach-to-filling-in-missing-/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/natashamjaques/MultimodalAutoencoder target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/1sgi43eHeWNVIx-NlVfRsR4U0eWIjjAKYtW6lgFHQngw/edit?usp=sharing" target=_blank rel=noopener>Slides</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/multimodal-autoencoder-a-deep-learning-approach-to-filling-in-missing-/><img src=/publication/multimodal-autoencoder-a-deep-learning-approach-to-filling-in-missing-/compact_hu3fe3ad9fe6ef5b7415513152a12330fa_280332_300x0_resize_lanczos_3.png alt="Multimodal Autoencoder: A Deep Learning Approach to Filling in Missing Sensor Data and Enabling Better Mood Prediction" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Generalization js-id-Healthcare js-id-Wellbeing js-id-Multi-task-Learning js-id-Machine-Learning js-id-Deep-Learning js-id-Hierarchical-Bayes js-id-Kernel-Methods"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/>Personalized Multitask Learning for Predicting Tomorrow's Mood, Stress, and Health</a></div><a href=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/ class=summary-link><div class=article-style>Traditional, one-size-fits-all machine learning models fail to account for individual differences in predicting wellbeing outcomes like stress, mood, and health. Instead, we personalize models to the individual using multi-task learning (MTL), employing hierarchical Bayes, kernel-based and deep neural network MTL models to improve prediction accuracy by 13-23%.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>S. Taylor</span><span>*</span>, <span>E. Nosakhare</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2017</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>IEEE Transactions on Affective Computing (TAFFC)</em> <strong>Best Paper</strong>; <em>NeurIPS Machine Learning for Healthcare (ML4HC) Workshop</em> <strong>Best Paper</strong></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mitmedialab/PersonalizedMultitaskLearning target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=gFXVGGzxQvU" target=_blank rel=noopener>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://pdfs.semanticscholar.org/b228/7a406985980515d5cc63e9b37fb17c5186f8.pdf target=_blank rel=noopener>ML4HC Best Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/pdfs/17.TaylorJaques-PredictingTomorrowsMoods.pdf target=_blank rel=noopener>TAFFC Journal Best Paper</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/><img src=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/compact_hud33baf28447e2e2dbecfa802753044ed_198069_300x0_resize_lanczos_3.png alt="Personalized Multitask Learning for Predicting Tomorrow's Mood, Stress, and Health" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Generalization js-id-Healthcare js-id-Multi-task-Learning js-id-Machine-Learning js-id-Deep-Learning js-id-Gaussian-Processes"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/predicting-tomorrows-mood-health-and-stress-level-using-personalized-m/>Predicting Tomorrow’s Mood, Health, and Stress Level using Personalized Multitask Learning and Domain Adaptation</a></div><a href=/publication/predicting-tomorrows-mood-health-and-stress-level-using-personalized-m/ class=summary-link><div class=article-style>Modeling measures like mood, stress, and health using a monolithic machine learning model leads to low prediction accuracy. Instead, we develop personalized regression models using multi-task learning and Gaussian Processes, leading to dramatic improvements in next-day predictions.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>O. Rudovic</span>, <span>S. Taylor</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2017</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Proceedings of Machine Learning Research</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://proceedings.mlr.press/v66/jaques17a/jaques17a.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/predicting-tomorrows-mood-health-and-stress-level-using-personalized-m/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/1cg9zK49a4ADTMpxv63zEYJgU44l7aQ2gOp_FSdd0A3k/edit?usp=sharing" target=_blank rel=noopener>Slides</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/predicting-tomorrows-mood-health-and-stress-level-using-personalized-m/><img src=/publication/predicting-tomorrows-mood-health-and-stress-level-using-personalized-m/compact_hu07571ff498d15a20626e0024e5f9b59d_190694_300x0_resize_lanczos_3.png alt="Predicting Tomorrow’s Mood, Health, and Stress Level using Personalized Multitask Learning and Domain Adaptation" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Sequence-Modeling js-id-Communication-and-Language js-id-Music-Generation js-id-Drug-Discovery js-id-Healthcare js-id-Generalization js-id-Transfer-Learning js-id-KL-control js-id-Reinforcement-Learning js-id-Machine-Learning js-id-Deep-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/sequence-tutor-conservative-finetuning-of-sequence-generation-models-w/>Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control</a></div><a href=/publication/sequence-tutor-conservative-finetuning-of-sequence-generation-models-w/ class=summary-link><div class=article-style>To combine supervised learning on data with reinforcement learning, we pre-train a supervised data prior, and penalize KL-divergence from this model using RL training. This enables effective learning of complex sequence-modeling problems for which we wish to match the data while optimizing external metrics like drug effectiveness. The approach produces compelling results in the disparate domains of music generation and drug discovery.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>S. Gu</span>, <span>D. Bahdanau</span>, <span>J. M. Hernandez-Lobato</span>, <span>R. E. Turner</span>, <span>D. Eck</span></div><span class=article-date>2017</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference on Machine Learning (ICML)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1611.02796 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/sequence-tutor-conservative-finetuning-of-sequence-generation-models-w/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/tensorflow/magenta/tree/master/magenta/models/rl_tuner target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://vimeo.com/240608475 target=_blank rel=noopener>ICML talk</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/abBfZB5DlSY target=_blank rel=noopener>Generated music</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://magenta.tensorflow.org/2016/11/09/tuning-recurrent-networks-with-reinforcement-learning target=_blank rel=noopener>Magenta blog</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.technologyreview.com/2016/11/30/155729/ai-songsmith-cranks-out-surprisingly-catchy-tunes/ target=_blank rel=noopener>MIT Tech Review article</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/sequence-tutor-conservative-finetuning-of-sequence-generation-models-w/><img src=/publication/sequence-tutor-conservative-finetuning-of-sequence-generation-models-w/compact_hu516b72a52b0e20f8f6988d67cd0bf946_71842_300x0_resize_lanczos_3.png alt="Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Behavior-Change js-id-Wellbeing js-id-Human-Computer-Interaction"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/bitxbit-encouraging-behavior-change-with-n2-experiments/>BITxBIT: Encouraging Behavior Change with N=2 Experiments</a></div><a href=/publication/bitxbit-encouraging-behavior-change-with-n2-experiments/ class=summary-link><div class=article-style>To help promote behavior change, we leverage the power of social obligation, and conduct an experiment in which participants are paired together and asked to design a Behavioral Intervention Technology (BIT) customized to suit their partner’s behavior change goal.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>T. Rich</span>, <span>K. Dinakar</span>, <span>N. Farve</span>, <span>W.V. Chen</span>, <span>P. Maes</span>, <span>R. Picard</span></div><span class=article-date>2016</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Proceedings of the CHI Conference Extended Abstracts on Human Factors</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://affect.media.mit.edu/pdfs/16.Jaques-etal-CHI.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/bitxbit-encouraging-behavior-change-with-n2-experiments/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Viral-MediaLab/BitByBit target=_blank rel=noopener>Code</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/bitxbit-encouraging-behavior-change-with-n2-experiments/><img src=/publication/bitxbit-encouraging-behavior-change-with-n2-experiments/compact_hu689cef82a8ea011d5247054f1d69a63e_91538_300x0_resize_lanczos_3.png alt="BITxBIT: Encouraging Behavior Change with N=2 Experiments" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Music-Generation js-id-Demo js-id-Deep-Learning js-id-Generative-Models js-id-Sequence-Modeling js-id-Reinforcement-Learning js-id-Human-AI-Interaction"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/interactive-musical-improvisation-with-magenta/>Interactive Musical Improvisation with Magenta</a></div><a href=/publication/interactive-musical-improvisation-with-magenta/ class=summary-link><div class=article-style>This demo deployed RL Tuner and other Magenta music generation models into an interactive interface in which users can collaborate creatively with a machine learning model. The interface supports call and response interaction, automatically generating an accompaniment to the user&rsquo;s melody, or melody morphing: responding both with variations on the user&rsquo;s melody and a bass accompaniment.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Roberts</span>, <span>J. Engel</span>, <span>C. Hawthorne</span>, <span>I. Simon</span>, <span>E. Waite</span>, <span>S. Oore</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>C. Resnick</span>, <span>D. Eck</span></div><span class=article-date>2016</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS)</em> <strong>Best Demo</strong></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/interactive-musical-improvisation-with-magenta/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/magenta/magenta target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=QlVoR1jQrPk&t=1s" target=_blank rel=noopener>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://nips.cc/Conferences/2016/Schedule?showEvent=6307" target=_blank rel=noopener>NeurIPS Demo</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://magenta.tensorflow.org/ target=_blank rel=noopener>Magenta</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://magenta.tensorflow.org/2016/12/16/nips-demo target=_blank rel=noopener>Blog post</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/interactive-musical-improvisation-with-magenta/><img src=/publication/interactive-musical-improvisation-with-magenta/compact_hue937982e008a1df44b297e8de8d4e977_857119_300x0_resize_lanczos_3.png alt="Interactive Musical Improvisation with Magenta" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Wellbeing js-id-Healthcare js-id-Physiology js-id-Machine-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/machine-learning-of-sleep-and-wake-behaviors-to-classify-selfreported-/>Machine Learning of Sleep and Wake Behaviors to Classify Self-Reported Evening Mood</a></div><a href=/publication/machine-learning-of-sleep-and-wake-behaviors-to-classify-selfreported-/ class=summary-link><div class=article-style>Machine learning applied to nightly data from sensors and smartphones, shows value for predicting college student’s mood the following evening. Using multi-task learning to simultaneously predicted related wellbeing factors like health, energy, stress, and alertness improves performance.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>S. Taylor</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>A. Sano</span>, <span>A. Azaria</span>, <span>A. Ghandeharioun</span>, <span>R. Picard</span></div><span class=article-date>2016</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Sleep</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/pdfs/16.Taylor-ClassifyingSelfReportedMood-SLEEP2016.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/machine-learning-of-sleep-and-wake-behaviors-to-classify-selfreported-/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mitmedialab/PersonalizedMultitaskLearning target=_blank rel=noopener>Code</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/machine-learning-of-sleep-and-wake-behaviors-to-classify-selfreported-/><img src=/publication/machine-learning-of-sleep-and-wake-behaviors-to-classify-selfreported-/compact_hu67a9313843bd4d0757512aa17e4e037d_288870_300x0_resize_lanczos_3.png alt="Machine Learning of Sleep and Wake Behaviors to Classify Self-Reported Evening Mood" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Communication-and-Language js-id-Affective-Computing js-id-Human-AI-Interaction js-id-Machine-Learning js-id-Psychology js-id-Intelligent-Virtual-Agents"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/personality-attitudes-and-bonding-in-conversations/>Personality, Attitudes, and Bonding in Conversations</a></div><a href=/publication/personality-attitudes-and-bonding-in-conversations/ class=summary-link><div class=article-style>We collect observational data from real human conversations, and develop a measure of how much participants experienced bonding or chemistry. We analyze the effects of personality and attitudes on bonding, and find that attentiveness and excitement are more effective at promoting bonding than traits like attractiveness and humour.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>Y. K. Kim</span>, <span>\& Picard R. Picard R</span></div><span class=article-date>2016</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Intelligent Virtual Agents (IVA)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://affect.media.mit.edu/pdfs/16.Jaques-IVApersonality.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/personality-attitudes-and-bonding-in-conversations/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/personality-attitudes-and-bonding-in-conversations/><img src=/publication/personality-attitudes-and-bonding-in-conversations/compact_hu6e52db2b195bd435c65d794401961075_82784_300x0_resize_lanczos_3.png alt="Personality, Attitudes, and Bonding in Conversations" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Music-Generation js-id-Generalization js-id-Generative-Models js-id-Reinforcement-Learning js-id-Deep-Learning js-id-Sequence-Modeling"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/>Tuning Recurrent Neural Networks with Reinforcement Learning</a></div><a href=/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/ class=summary-link><div class=article-style>Generating music using traditional supervised sequence models suffers from known failure modes, including the inability to produce coherent global structure. Music is an interesting sequence generation problem, because musical compositions adhere to known rules. We impose these rules with a novel algorithm combining RL and supervised learning.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>S. Gu</span>, <span>R. E. Turner</span>, <span>D. Eck</span></div><span class=article-date>2016</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference on Learning Representations (ICLR) - workshop</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/pdf?id=Syyv2e-Kx" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/tensorflow/magenta/tree/master/magenta/models/rl_tuner target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://magenta.tensorflow.org/2016/11/09/tuning-recurrent-networks-with-reinforcement-learning target=_blank rel=noopener>Magenta blog</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.technologyreview.com/2016/11/30/155729/ai-songsmith-cranks-out-surprisingly-catchy-tunes/ target=_blank rel=noopener>MIT Tech Review article</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/><img src=/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/compact_hu5c8d57118dda98d2151abb98b5a96367_50040_300x0_resize_lanczos_3.png alt="Tuning Recurrent Neural Networks with Reinforcement Learning" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Human-AI-Interaction js-id-Intelligent-Virtual-Agents js-id-Machine-Learning js-id-Deep-Learning js-id-Communication-and-Language"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/understanding-and-predicting-bonding-in-conversations-using-thin-slice/>Understanding and Predicting Bonding in Conversations Using Thin Slices of Facial Expressions and Body Language</a></div><a href=/publication/understanding-and-predicting-bonding-in-conversations-using-thin-slice/ class=summary-link><div class=article-style>Given only one-minute slices of facial expressions and body language, we use machine learning to accurately predict whether two humans having a conversation will bond with each other. We analyze factors which lead to bonding and discover that synchrony in body language and appropriate, empathetic facial expressions lead to higher bonding.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>D. McDuff</span>, <span>Y. K. Kim</span>, <span>\& Picard R. Picard R</span></div><span class=article-date>2016</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Intelligent Virtual Agents (IVA)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://affect.media.mit.edu/pdfs/16.Jaques-IVAbonding.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/understanding-and-predicting-bonding-in-conversations-using-thin-slice/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/understanding-and-predicting-bonding-in-conversations-using-thin-slice/><img src=/publication/understanding-and-predicting-bonding-in-conversations-using-thin-slice/compact_hu695472130ff302a84610d3bb9619ffff_126124_300x0_resize_lanczos_3.png alt="Understanding and Predicting Bonding in Conversations Using Thin Slices of Facial Expressions and Body Language" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Active-Learning js-id-Physiology js-id-Affective-Computing js-id-Sensors js-id-Machine-Learning js-id-Electrodermal-Activity"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/active-learning-for-electrodermal-activity-classification/>Active learning for Electrodermal Activity classification</a></div><a href=/publication/active-learning-for-electrodermal-activity-classification/ class=summary-link><div class=article-style>We use labels provided by domain experts to classify whether artifacts are present in an Electrodermal Activity signal. Through the use of active learning, we improve sample efficiency and reduce the burden on human experts by as much as 84%, while offering the same or improved performance.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>V. Xia</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>S. Taylor</span>, <span>S. Fedor</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>IEEE Conference on Signal Processing in Medicine and Biology (SPMB)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dspace.mit.edu/openaccess-disseminate/1721.1/109392 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/active-learning-for-electrodermal-activity-classification/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/active-learning-for-electrodermal-activity-classification/><img src=/publication/active-learning-for-electrodermal-activity-classification/compact_hu5d816a23bf8e0beebb3ea391d5419449_46952_300x0_resize_lanczos_3.png alt="Active learning for Electrodermal Activity classification" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Physiology js-id-Sensors js-id-Machine-Learning js-id-Electrodermal-Activity"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/automatic-identification-of-artifacts-in-electrodermal-activity-data/>Automatic identification of artifacts in Electrodermal Activity data</a></div><a href=/publication/automatic-identification-of-artifacts-in-electrodermal-activity-data/ class=summary-link><div class=article-style>Ambulatory measurement of Electrodermal Activity (EDA) from the wrist has important clinical benefits, such as predicting mood, stress, health, or even seizures. However, ambulatory measurement is noisy, and artifacts can easily be mistaken for true Skin Conductance Responses (SCRs). In addition to our paper which describes a machine learning method for detecting artifacts with 95% test accuracy, we built EDA Explorer, an open-source tool that allows users to automatically detect artifacts and SCRs within their data.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>S. Taylor</span><span>*</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>W. Chen</span>, <span>S. Fedor</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5413200/ target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/automatic-identification-of-artifacts-in-electrodermal-activity-data/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/MITMediaLabAffectiveComputing/eda-explorer target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://eda-explorer.media.mit.edu/ target=_blank rel=noopener>EDA Explorer tool</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=s_QqG-QtMdM" target=_blank rel=noopener>Artifact detection tutorial</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=BbnOmQzxBh4" target=_blank rel=noopener>SCR detection tutorial</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://eda-explorer.media.mit.edu/research/ target=_blank rel=noopener>Research which uses EDA Explorer</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/automatic-identification-of-artifacts-in-electrodermal-activity-data/><img src=/publication/automatic-identification-of-artifacts-in-electrodermal-activity-data/compact_hu7294be0f4dfdda5d2236b827ed850b64_89306_300x0_resize_lanczos_3.png alt="Automatic identification of artifacts in Electrodermal Activity data" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Behavior-Change js-id-Wellbeing js-id-Affective-Computing js-id-Human-Computer-Interaction js-id-Network-Analysis"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/engaging-the-workplace-with-challenges/>Engaging the workplace with challenges</a></div><a href=/publication/engaging-the-workplace-with-challenges/ class=summary-link><div class=article-style>The Challenge is a tool aimed at promoting social connections and decreasing sedentary activity in a workplace environment. Participants are paired with a partner to complete short physical challenges, leveraging social obligation and social consensus to drive behavior change.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>N. Farve</span></div><span class=article-date>2015</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference on Persuasive Technologies</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/15Pimm1FwwxSPPX04KTr84ilVHJvVExPd/view?usp=sharing" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/engaging-the-workplace-with-challenges/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=coyW2yzQhFg" target=_blank rel=noopener>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/15Pimm1FwwxSPPX04KTr84ilVHJvVExPd/view?usp=sharing" target=_blank rel=noopener>Extended paper</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/engaging-the-workplace-with-challenges/><img src=/publication/engaging-the-workplace-with-challenges/compact_hu10f79fef9b503b31f453628fb0c5ea45_416386_300x0_resize_lanczos_3.png alt="Engaging the workplace with challenges" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Generalization js-id-Affective-Computing js-id-Wellbeing js-id-Healthcare js-id-Physiology js-id-Multi-task-Learning js-id-Machine-Learning js-id-Kernel-Methods"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/multitask-multikernel-learning-for-estimating-individual-wellbeing/>Multi-task Multi-Kernel Learning for Estimating Individual Wellbeing</a></div><a href=/publication/multitask-multikernel-learning-for-estimating-individual-wellbeing/ class=summary-link><div class=article-style>Wellbeing is a complex internal state consisting of several related dimensions, such as happiness, stress, energy, and health. We use Multi-task Multi-kernel learning to classify them simultaneously, leading to significant performance approvements.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>S. Taylor</span><span>*</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS) Workshop on Multimodal Machine Learning</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/pdfs/15.Jaques-etal-NIPSMMML.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/multitask-multikernel-learning-for-estimating-individual-wellbeing/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mitmedialab/PersonalizedMultitaskLearning/tree/master/MTMKL target=_blank rel=noopener>Code</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/multitask-multikernel-learning-for-estimating-individual-wellbeing/><img src=/publication/multitask-multikernel-learning-for-estimating-individual-wellbeing/compact_hu239a8902a06eac83286a213ce87b3050_96885_300x0_resize_lanczos_3.png alt="Multi-task Multi-Kernel Learning for Estimating Individual Wellbeing" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Healthcare js-id-Physiology js-id-Sensors js-id-Machine-Learning js-id-Kernel-Methods"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/predicting-students-happiness-from-physiology-phone-mobility-and-behav/>Predicting students' happiness from physiology, phone, mobility, and behavioral data</a></div><a href=/publication/predicting-students-happiness-from-physiology-phone-mobility-and-behav/ class=summary-link><div class=article-style>We train machine learning models to predict students&rsquo; happiness from extensive data comprising physiological signals, location, smartphone logs, and behavioral questions. Analyzing which features provide the highest information gain reveals that skin conductance during sleep, social interaction, exercise, and fewer phone screen hours are all positively associated with happiness.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>S. Taylor</span><span>*</span>, <span>A. Azaria</span>, <span>A. Ghandeharioun</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference on Affective Computing and Intelligent Interaction (ACII)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dam-prod.media.mit.edu/x/files/pdfs/15.Jaques-Taylor-et-al-PredictingHappiness.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/predicting-students-happiness-from-physiology-phone-mobility-and-behav/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5431070/ target=_blank rel=noopener>NCBI link</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/predicting-students-happiness-from-physiology-phone-mobility-and-behav/><img src=/publication/predicting-students-happiness-from-physiology-phone-mobility-and-behav/compact_hu1fc4810ff4bc072cc6b18f280751529d_106282_300x0_resize_lanczos_3.png alt="Predicting students' happiness from physiology, phone, mobility, and behavioral data" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Healthcare js-id-Physiology js-id-Sensors js-id-Machine-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/prediction-of-happysad-mood-from-daily-behaviors-and-previous-sleep-hi/>Prediction of happy-sad mood from daily behaviors and previous sleep history</a></div><a href=/publication/prediction-of-happysad-mood-from-daily-behaviors-and-previous-sleep-hi/ class=summary-link><div class=article-style>We trained machine learning models to classify happy vs. sad moods in college students using data from surveys and wearable sensors. Factors such as poor health-related behavior, more academic activity hours, and more neutral social interactions were highly predictive of mood.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Sano</span>, <span>A. Z. Yu</span>, <span>A. W. McHill</span>, <span>A.J. Phillips</span>, <span>S. Taylor</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>C. A. Czeisler</span>, <span>E. B. Klerman</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4768795/ target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/prediction-of-happysad-mood-from-daily-behaviors-and-previous-sleep-hi/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/prediction-of-happysad-mood-from-daily-behaviors-and-previous-sleep-hi/><img src=/publication/prediction-of-happysad-mood-from-daily-behaviors-and-previous-sleep-hi/compact_hu9e6df34fa365a53cb9c7ef5c87f076ed_40749_300x0_resize_lanczos_3.png alt="Prediction of happy-sad mood from daily behaviors and previous sleep history" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Healthcare js-id-Physiology js-id-Sensors js-id-Machine-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/recognizing-academic-performance-sleep-quality-stress-level-and-mental/>Recognizing academic performance, sleep quality, stress level, and mental health using personality traits, wearable sensors and mobile phones</a></div><a href=/publication/recognizing-academic-performance-sleep-quality-stress-level-and-mental/ class=summary-link><div class=article-style>SNAPSHOT was a large-scale study of college undergraduates which tracked detailed longitudinal data from smartphones, wearable sensors, behavioral data, and mental health and sleep quality surveys. This initial study analyzed relationships between sleep quality, stress, and GPA, and used machine learning to predict these indices from objective phone and sensor data.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Sano</span>, <span>A. J. Phillips</span>, <span>A. Z. Yu</span>, <span>A. W. McHill</span>, <span>S. Taylor</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>C. A. Czeisler</span>, <span>E. B. Klerman</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Wearable and Implantable Body Sensor Networks (BSN)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5431072/ target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/recognizing-academic-performance-sleep-quality-stress-level-and-mental/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/recognizing-academic-performance-sleep-quality-stress-level-and-mental/><img src=/publication/recognizing-academic-performance-sleep-quality-stress-level-and-mental/compact_hu5ce76c7729fa9c87a12cdf6d14a5f3dd_65628_300x0_resize_q75_lanczos.jpg alt="Recognizing academic performance, sleep quality, stress level, and mental health using personality traits, wearable sensors and mobile phones" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Human-AI-Interaction js-id-Affective-Computing js-id-Wellbeing js-id-Human-Computer-Interaction"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/smiletracker-automatically-and-unobtrusively-recording-smiles-and-thei/>SmileTracker: Automatically and Unobtrusively Recording Smiles and their Context.</a></div><a href=/publication/smiletracker-automatically-and-unobtrusively-recording-smiles-and-thei/ class=summary-link><div class=article-style>SmileTracker is an app that uses facial expression recognition to take a screenshot of the user&rsquo;s screen whenever they smile. The screenshot and image of the user&rsquo;s face are saved, to help them remember positive content they encountered during the day. Users can opt to share their images to a public gallery.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>W. V. Chen</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Proceedings of the CHI Conference Extended Abstracts</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/pdfs/15.jaques-chen-picard-CHI.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/smiletracker-automatically-and-unobtrusively-recording-smiles-and-thei/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/11prrldwZsdM7wjtZRgTJggSm1HGNxOxCoYfxnOwepX8/edit?usp=sharing" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.bostonmagazine.com/news/2015/01/05/smiletracker-captures-photos-internet/ target=_blank rel=noopener>Boston Magazine article</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.cbc.ca/news/canada/saskatchewan/regina-woman-develops-smile-app-at-mit-1.2886943 target=_blank rel=noopener>CBC news</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/smiletracker-automatically-and-unobtrusively-recording-smiles-and-thei/><img src=/publication/smiletracker-automatically-and-unobtrusively-recording-smiles-and-thei/compact_hub0db268a58f10c1ddc7d4c8f51cd96d7_905411_300x0_resize_lanczos_3.png alt="SmileTracker: Automatically and Unobtrusively Recording Smiles and their Context." loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Physiology js-id-Sensors js-id-Affective-Computing js-id-Healthcare js-id-Signal-Processing js-id-Electrodermal-Activity js-id-Machine-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/waveletbased-motion-artifact-removal-for-electrodermal-activity/>Wavelet-based motion artifact removal for Electrodermal Activity</a></div><a href=/publication/waveletbased-motion-artifact-removal-for-electrodermal-activity/ class=summary-link><div class=article-style>We propose a method for removing motion artifacts from Electrodermal Activity using a stationary wavelet transform. We modeled the wavelet coefficients as a Gaussian mixture distribution corresponding to the underlying skin conductance level and skin conductance responses. Our method achieves a greater reduction of artifacts while retaining motion-artifact-free data.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>W. Chen</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>S. Taylor</span>, <span>A. Sano</span>, <span>S. Fedor</span>, <span>\& Picard R. Picard R</span></div><span class=article-date>2015</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5413204/ target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/waveletbased-motion-artifact-removal-for-electrodermal-activity/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/waveletbased-motion-artifact-removal-for-electrodermal-activity/><img src=/publication/waveletbased-motion-artifact-removal-for-electrodermal-activity/compact_hu0bf3160fd52e2ce5026eacf9f44663d5_43297_300x0_resize_q75_lanczos.jpg alt="Wavelet-based motion artifact removal for Electrodermal Activity" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Intelligent-Tutoring-Systems js-id-Human-AI-Interaction js-id-Affective-Computing js-id-Machine-Learning js-id-Eye-Tracking"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/predicting-affect-from-gaze-data-during-interaction-with-an-intelligen/>Predicting Affect from Gaze Data During Interaction with an Intelligent Tutoring System</a></div><a href=/publication/predicting-affect-from-gaze-data-during-interaction-with-an-intelligen/ class=summary-link><div class=article-style>Using eye-tracking data collected while students interact with an Intelligent Tutoring System, we train machine learning models to predict when students are experiencing boredom and curiosity. Which analyze which features are most relevant to detecting when students are engaged and curious vs. disengaged and bored.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>C. Conati</span>, <span>J. M. Harley</span>, <span>R. Azevedo</span></div><span class=article-date>2014</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Intelligent Tutoring Systems</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://www.cs.ubc.ca/~conati/my-papers/ITS-Natasha-2014.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/predicting-affect-from-gaze-data-during-interaction-with-an-intelligen/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/1qDmD-GfFIVNunBlrq5otj0lxEpg0h0f2YC8Sa9dBnyo/edit?usp=sharing" target=_blank rel=noopener>Slides</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/predicting-affect-from-gaze-data-during-interaction-with-an-intelligen/><img src=/publication/predicting-affect-from-gaze-data-during-interaction-with-an-intelligen/compact_hu4e77df6e59970660e764b5e2d83fe42d_407528_300x0_resize_lanczos_3.png alt="Predicting Affect from Gaze Data During Interaction with an Intelligent Tutoring System" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Intelligent-Tutoring-Systems js-id-Human-AI-Interaction js-id-Physiology js-id-Eye-Tracking js-id-Machine-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/predicting-affect-in-an-intelligent-tutoring-system/>Predicting Affect in an Intelligent Tutoring System</a></div><a href=/publication/predicting-affect-in-an-intelligent-tutoring-system/ class=summary-link><div class=article-style>My Master&rsquo;s Thesis investigated the usefulness of different data sources for automatically predicting when students using an Intelligent Tutoring System were engaged and curious, or disengaged and bored. Detailed comparisons of machine learning algorithms trained with eye-tracking data, Electrodermal Activity (EDA) and distance from the screen revealed that distance (which can be obtained with cheap infra-red sensors) provided one of the simplest and most reliable signals of student engagement.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span></div><span class=article-date>2014</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>University of British Columbia</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://open.library.ubc.ca/collections/ubctheses/24/items/1.0135541 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/predicting-affect-in-an-intelligent-tutoring-system/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/1yPA8ollN0mNg-reAZF2kMhXs9txXrHx2UkLauWFwdd4/edit?usp=sharing" target=_blank rel=noopener>Slides</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/predicting-affect-in-an-intelligent-tutoring-system/><img src=/publication/predicting-affect-in-an-intelligent-tutoring-system/compact_huce6bce66b091a213317cf9cc400e25e6_83700_300x0_resize_lanczos_3.png alt="Predicting Affect in an Intelligent Tutoring System" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Gesture-Recognition js-id-Deep-Learning js-id-Machine-Learning js-id-Human-AI-Interaction"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/a-comparison-of-random-forests-and-dropout-nets-for-sign-language-reco/>A Comparison of Random Forests and Dropout Nets for Sign Language Recognition with the Kinect</a></div><a href=/publication/a-comparison-of-random-forests-and-dropout-nets-for-sign-language-reco/ class=summary-link><div class=article-style>We conduct a study in which participants form American Sign Language hand signs while being recorded with a Microsoft Kinect. The resulting infra-red distance data are used to train both neural networks with dropout (dropout NN) and Random Forests; dropout NN perform significantly better.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>J. Nutini</span></div><span class=article-date>2013</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Unpublished manuscript</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1F9X_xhKUYDJwuz9Eql548OlRpUW1M63X/view?usp=sharing" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/a-comparison-of-random-forests-and-dropout-nets-for-sign-language-reco/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/a-comparison-of-random-forests-and-dropout-nets-for-sign-language-reco/><img src=/publication/a-comparison-of-random-forests-and-dropout-nets-for-sign-language-reco/compact_hu78c067b14e65a1cb778dd02730c0c929_174121_300x0_resize_lanczos_3.png alt="A Comparison of Random Forests and Dropout Nets for Sign Language Recognition with the Kinect" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Intelligent-Tutoring-Systems js-id-Human-AI-Interaction js-id-Affective-Computing js-id-Machine-Learning js-id-Reinforcement-Learning"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/emotionally-adaptive-intelligent-tutoring-systems-using-pomdps/>Emotionally Adaptive Intelligent Tutoring Systems using POMDPs</a></div><a href=/publication/emotionally-adaptive-intelligent-tutoring-systems-using-pomdps/ class=summary-link><div class=article-style>An emerging field in user-adaptive systems is affect adaptivity: modeling and responding to an estimation of the user’s emotional state. Prior work used Dynamic Bayesian Networks to obtain adaptivity, but in this paper we represent the problem as a Partially Observable Markov Decision Process (POMDP) and find solutions that compute a plan of interventions for an Intelligent Tutoring System to take given an estimation of the user’s mood and goals.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span></div><span class=article-date>2013</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Unpublished manuscript</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1oKVJwaU0hwSeu1oUWQSH9W29eUj-j_gm/view?usp=sharing" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/emotionally-adaptive-intelligent-tutoring-systems-using-pomdps/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/emotionally-adaptive-intelligent-tutoring-systems-using-pomdps/><img src=/publication/emotionally-adaptive-intelligent-tutoring-systems-using-pomdps/compact_huf02ba020db9d2da528e102de3496f456_444787_300x0_resize_lanczos_3.png alt="Emotionally Adaptive Intelligent Tutoring Systems using POMDPs" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Machine-Learning js-id-Compression js-id-Affective-Computing"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/fast-johnsonlindenstrauss-transform-for-classification-of-high-dimensi/>Fast Johnson–Lindenstrauss transform for classification of high dimensional data</a></div><a href=/publication/fast-johnsonlindenstrauss-transform-for-classification-of-high-dimensi/ class=summary-link><div class=article-style>This paper investigates the utility of using the Fast Johnson-Lindenstrauss Transform to produce a low-dimensional random projection of eye-tracking data features that can be used for classifying emotion in an Intelligent Tutoring System. Interestingly, the FJLT provides similar or superior performance to more computationally expensive techniques.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span></div><span class=article-date>2013</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>Unpublished manuscript</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1qBSWgePvclrjiQMVz2ir4w8wM1cqNU__/view?usp=sharing" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/fast-johnsonlindenstrauss-transform-for-classification-of-high-dimensi/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/fast-johnsonlindenstrauss-transform-for-classification-of-high-dimensi/><img src=/publication/fast-johnsonlindenstrauss-transform-for-classification-of-high-dimensi/compact_hudc1b4ad197a6b0a04a0c6685b1f1b04d_76816_300x0_resize_lanczos_3.png alt="Fast Johnson–Lindenstrauss transform for classification of high dimensional data" loading=lazy></a></div></div></div></div><div class="col-12 isotope-item js-id-Intelligent-Tutoring-Systems js-id-Human-AI-Interaction js-id-Machine-Learning js-id-Affective-Computing"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/understanding-attention-to-adaptive-hints-in-educational-games-an-eyet/>Understanding attention to adaptive hints in educational games: an eye-tracking study</a></div><a href=/publication/understanding-attention-to-adaptive-hints-in-educational-games-an-eyet/ class=summary-link><div class=article-style>This study uses eye tracking to assess how students interact with automatic, adaptive hints in an Intelligent Tutoring System. Specifically, we study Prime Climb, an educational game which provides individualized support for learning number factorization skills in the form of hints generated from a model of student learning.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>C. Conati</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>M. Muir</span></div><span class=article-date>2013</span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>International Journal of Artificial Intelligence in Education</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://link.springer.com/article/10.1007/s40593-013-0002-8 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/understanding-attention-to-adaptive-hints-in-educational-games-an-eyet/cite.bib>Cite</a></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/publication/understanding-attention-to-adaptive-hints-in-educational-games-an-eyet/><img src=/publication/understanding-attention-to-adaptive-hints-in-educational-games-an-eyet/compact_hu0f844d9ed0f15d724f3012adef3f85da_841661_300x0_resize_lanczos_3.png alt="Understanding attention to adaptive hints in educational games: an eye-tracking study" loading=lazy></a></div></div></div></div></div></div></div></div></section><section id=tags class="home-section wg-tag-cloud"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Popular Topics</h1></div><div class="col-12 col-lg-8"><div class=tag-cloud><a href=/tag/affective-computing/ style=font-size:1.9798919880521804rem>Affective Computing</a>
<a href=/tag/communication-and-language/ style=font-size:1.3295263086532545rem>Communication and Language</a>
<a href=/tag/cooperation/ style=font-size:.7rem>Cooperation</a>
<a href=/tag/deep-learning/ style=font-size:1.8671731456651695rem>Deep Learning</a>
<a href=/tag/electrodermal-activity/ style=font-size:.7rem>Electrodermal Activity</a>
<a href=/tag/emergent-complexity/ style=font-size:.7rem>Emergent Complexity</a>
<a href=/tag/generalization/ style=font-size:1.4445145383246347rem>Generalization</a>
<a href=/tag/generative-models/ style=font-size:.7rem>Generative Models</a>
<a href=/tag/healthcare/ style=font-size:1.4943737575706941rem>Healthcare</a>
<a href=/tag/human-ai-interaction/ style=font-size:1.5827051092668336rem>Human-AI Interaction</a>
<a href=/tag/human-computer-interaction/ style=font-size:.7rem>Human-Computer Interaction</a>
<a href=/tag/intelligent-tutoring-systems/ style=font-size:.8648474489174397rem>Intelligent Tutoring Systems</a>
<a href=/tag/machine-learning/ style=font-size:1.9798919880521804rem>Machine Learning</a>
<a href=/tag/multi-agent/ style=font-size:1.2620343277027866rem>Multi-Agent</a>
<a href=/tag/physiology/ style=font-size:1.4445145383246347rem>Physiology</a>
<a href=/tag/reinforcement-learning/ style=font-size:1.6222393727723183rem>Reinforcement Learning</a>
<a href=/tag/sensors/ style=font-size:1.3295263086532545rem>Sensors</a>
<a href=/tag/sequence-modeling/ style=font-size:.8648474489174397rem>Sequence Modeling</a>
<a href=/tag/social-learning/ style=font-size:1.097186878785347rem>Social Learning</a>
<a href=/tag/wellbeing/ style=font-size:1.2620343277027866rem>Wellbeing</a></div></div></div></div></section><section id=talks class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Featured Talks</h1></div><div class="col-12 col-lg-8"><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/conference-on-robot-learning-corl-tutorial/>Conference on Robot Learning (CoRL) Tutorial</a></div><a href=/talk/conference-on-robot-learning-corl-tutorial/ class=summary-link><div class=article-style>An overview of Social Reinforcement Learning, including multi-agent coordination, and using multi-agent training as a tool to induce emergent complexity and improve generalization.</div></a><div class="stream-meta article-metadata"><div><span>2021</span>
<span class=middot-divider></span>
<span>Virtual</span></div></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><iframe width=300 src=https://www.youtube.com/embed/5KjpZS4_RBs title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div></div><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/mila-rl-sofa/>MILA RL Sofa</a></div><a href=/talk/mila-rl-sofa/ class=summary-link><div class=article-style>This talk focuses on how social learning from other agents can lead to learning more complex behaviors and enhance generalization. I discuss recent work on emergent social learning and PsiPhi-Learning in detail.</div></a><div class="stream-meta article-metadata"><div><span>2021</span>
<span class=middot-divider></span>
<span>Montreal Institute of Learning Algorithms (MILA)</span></div></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><a href=/talk/mila-rl-sofa/><img src=/talk/mila-rl-sofa/featured_huea03fa0c9747c3b8efd220aac3b6f6c6_1299935_300x0_resize_lanczos_3.png alt="MILA RL Sofa" loading=lazy></a></div></div></div><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/re-work-women-in-ai-podcast/>Re-Work Women in AI Podcast</a></div><a href=/talk/re-work-women-in-ai-podcast/ class=summary-link><div class=article-style>A fun discussion of my research, career trajectory, and take on possible beneficial future directions for reinforcement learning research.</div></a><div class="stream-meta article-metadata"><div><span>2021</span>
<span class=middot-divider></span>
<span>Re-Work</span></div></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><iframe width=300 src=https://www.youtube.com/embed/_gLU5Uw4TWU title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div></div><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/institute-of-cognitive-science-deep-reinforcement-learning-workshop/>Institute of Cognitive Science Deep Reinforcement Learning Workshop</a></div><a href=/talk/institute-of-cognitive-science-deep-reinforcement-learning-workshop/ class=summary-link><div class=article-style>In addition to talking about Social Reinforcement Learning, I participated in a panel discussion with Deepak Pathak.</div></a><div class="stream-meta article-metadata"><div><span>2020</span>
<span class=middot-divider></span>
<span>University of Osnabrück</span></div></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><iframe width=300 src=https://www.youtube.com/embed/G__GZKNhvsU title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div></div><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/ucl-deciding-acting-and-reasoning-with-knowledge-dark-seminar/>UCL Deciding, Acting, and Reasoning with Knowledge (DARK) Seminar</a></div><a href=/talk/ucl-deciding-acting-and-reasoning-with-knowledge-dark-seminar/ class=summary-link><div class=article-style>An overview of Social Reinforcement Learning including using multi-agent competition to drive emergent complexity via PAIRED, increasing multi-agent coordination with Social Influence, and learning from human feedback in dialog with Offline RL.</div></a><div class="stream-meta article-metadata"><div><span>2020</span>
<span class=middot-divider></span>
<span>University College London (UCL)</span></div></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><iframe width=300 src=https://www.youtube.com/embed/traKBhJm4lQ title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div></div><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/samsung-forum/>Samsung Forum</a></div><a href=/talk/samsung-forum/ class=summary-link><div class=article-style>A talk entitled &ldquo;Towards Social and Affective Machine Learning&rdquo;, which covers most of the same content as my PhD defense, including my early PhD work on using multi-task learning for personalized wellbeing prediction.</div></a><div class="stream-meta article-metadata"><div><span>2020</span>
<span class=middot-divider></span>
<span>Samsung Strategy & Innovation Center</span></div></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><iframe width=300 src=https://www.youtube.com/embed/_gLU5Uw4TWU title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div></div><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/thesis-defense/>Thesis Defense</a></div><a href=/talk/thesis-defense/ class=summary-link><div class=article-style>My thesis defense at the MIT Media Lab. I cover work on Affective Computing, learning from affective signals in human-AI interaction, and multi-agent coordination. Includes an in-depth question period with my PhD committee.</div></a><div class="stream-meta article-metadata"><div><span>2019</span>
<span class=middot-divider></span>
<span>Virtual</span></div></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><iframe width=300 src=https://www.youtube.com/embed/a-oWa2CS8jg title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div></div><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/broad-institute-models-inference-and-algorithms-mia-seminar/>Broad Institute Models, Inference, and Algorithms (MIA) Seminar</a></div><a href=/talk/broad-institute-models-inference-and-algorithms-mia-seminar/ class=summary-link><div class=article-style>This talk covers &ldquo;Mechanisms for Generalized Machine Learning Across Tasks and Environments&rdquo;, including a KL-control technique for combining reinforcement and supervised learning, and describes how to apply it to the problem of drug discovery.</div></a><div class="stream-meta article-metadata"><div><span>2019</span>
<span class=middot-divider></span>
<span>Broad Institute</span></div></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><iframe width=300 src=https://www.youtube.com/embed/Vx5Daxa0Yts title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div></div><div class=container><div class="row media stream-item"><div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body"><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/starsconf-keynote-lecture/>StarsConf Keynote Lecture</a></div><a href=/talk/starsconf-keynote-lecture/ class=summary-link><div class=article-style>A talk describing recent advances in AI and machine learning to a general tech audience, including some of my own work. For example, I discuss how to improve the output of a generative sketching model by monitoring people&rsquo;s facial expression reactions to samples from the model.</div></a><div class="stream-meta article-metadata"><div><span>2018</span>
<span class=middot-divider></span>
<span>Santiago, Chile</span></div></div></div><div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3"><iframe width=300 src=https://www.youtube.com/embed/QI5Gvn8FDG0 title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div></div></div></div></div></section><section id=communities class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Research Communities</h1></div><div class="col-12 col-lg-8"><ul><li>Together with Eugene Vinitsky, I run the <a href=https://sites.google.com/corp/view/berkeleymarl/home target=_blank rel=noopener>Berkeley Multi-Agent Reinforcement Learning Seminar</a>, which includes participants from Berkeley, Stanford, Google Brain, OpenAI, Facebook AI Research (FAIR), and other universities.</li><li>Co-organizer of the <a href=https://www.cooperativeai.com/neurips-2021/workshop-information target=_blank rel=noopener>NeurIPS 2021 Cooperative AI workshop</a>.<ul><li>As part of the workshop, I planned and implented a <a href=https://www.cooperativeai.com/neurips-2021/workshop-information#h.ssihuuiopnhu target=_blank rel=noopener>mentorship program</a> to provide feedback on submissions to students from underrepresented groups.</li></ul></li><li>Panelist and moderator for the <a href=https://www.cooperativeai.com/neurips-2020/speakers target=_blank rel=noopener>NeurIPS 2020 Cooperative AI workshop</a>.</li><li>Co-organizer of the <a href=https://www.climatechange.ai/events/iclr2020.html target=_blank rel=noopener>ICLR 2020 Climate Change for Artificial Intelligence (CCAI) workshop</a>.</li><li>Former <a href=https://www.climatechange.ai/about#people target=_blank rel=noopener>Social Media Lead for CCAI</a>.</li><li>Co-organizer of the <a href=https://sites.google.com/corp/view/emecom2019/home target=_blank rel=noopener>NeurIPS 2019 Emergent Communication (EmeComm) workshop</a>.</li><li>Co-organizer of the <a href="https://icml.cc/Conferences/2018/Schedule?showEvent=3304" target=_blank rel=noopener>ICML 2018 Artificial Intelligence in Affective Computing (AffComp) workshop</a>.</li></ul></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Built using <a href=https://gohugo.io target=_blank rel=noopener>Hugo</a> and the <a href=https://github.com/wowchemy/starter-hugo-academic target=_blank rel=noopener>Wowchemy academic template</a>. View <a href=https://github.com/natashamjaques/professional_website target=_blank rel=noopener>source</a>.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script>
<script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script>
<script src=/en/js/wowchemy.min.26bc5a5b73c468c9e767656a378ac5e3.js></script>
<script async defer src=https://buttons.github.io/buttons.js></script></body></html>