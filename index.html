<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.2.0 for Hugo"><meta name=author content="Natasha Jaques"><meta name=description content><link rel=alternate hreflang=en-us href=/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload="this.media='all'" disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload="this.media='all'"><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload="this.media='all'"><link rel=stylesheet href=/css/wowchemy.4aace023fb5b572c1b1b57368af1a441.css><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script><link rel=alternate href=/index.xml type=application/rss+xml title="Natasha Jaques"><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/media/icon_hua64261c49917fe48d46061d344453b21_144873_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/media/icon_hua64261c49917fe48d46061d344453b21_144873_180x180_fill_lanczos_center_2.png><link rel=canonical href=/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Natasha Jaques"><meta property="og:url" content="/"><meta property="og:title" content="Natasha Jaques"><meta property="og:description" content><meta property="og:image" content="/media/icon_hua64261c49917fe48d46061d344453b21_144873_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="/media/icon_hua64261c49917fe48d46061d344453b21_144873_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2030-06-01T13:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"?q={search_term_string}","query-input":"required name=search_term_string"},"url":""}</script><title>Natasha Jaques</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper><script src=/js/wowchemy-init.min.b8153d4570dcbb34350a2a846dba8c03.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Natasha Jaques</a></div><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Natasha Jaques</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://twitter.com/natashajaques data-toggle=tooltip data-placement=bottom title=Twitter target=_blank rel=noopener aria-label=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" src=/author/natasha-jaques/avatar_hu6e0b3d3a5886c72ee9eb657e72f07cfe_691297_270x270_fill_lanczos_center_2.png alt="Natasha Jaques"><div class=portrait-title><h2>Natasha Jaques</h2><h3><a href=https://research.google/people/NatashaJaques/ target=_blank rel=noopener><span>Research Scientist, Google Brain</span></a></h3><h3><a href=http://rail.eecs.berkeley.edu/people.html target=_blank rel=noopener><span>Postdoctoral Fellow, UC Berkeley</span></a></h3></div><ul class=network-icon aria-hidden=true><li><a href=mailto:natashamjaques@gmail.com aria-label=envelope><i class="fas fa-envelope big-icon"></i></a></li><li><a href=https://twitter.com/natashajaques target=_blank rel=noopener aria-label=twitter><i class="fab fa-twitter big-icon"></i></a></li><li><a href="https://scholar.google.com/citations?hl=en&user=8iCb2TwAAAAJ" target=_blank rel=noopener aria-label=graduation-cap><i class="fas fa-graduation-cap big-icon"></i></a></li><li><a href=https://github.com/natashamjaques target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><div class=article-style><p>Social learning helps humans and animals rapidly adapt to new circumstances, and drives the emergence of complex learned behaviors. My research is focused on <strong>Social Reinforcement Learning</strong>&mdash;developing algorithms that use insights from social learning and Affective Computing to improve AI agents' <a href=./publication/paired>learning</a>, <a href=./publication/learning-social-learning/>generalization</a>, <a href=./publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/>coordination</a>, and <a href=./publication/humancentric-dialog-training-via-offline-reinforcement-learning/>human-AI interaction</a>.</p><p>I currently hold a joint position as a <a href=https://research.google/people/NatashaJaques/ target=_blank rel=noopener>Research Scientist at Google Brain</a> and <a href=http://rail.eecs.berkeley.edu/people.html target=_blank rel=noopener>Postdoctoral Fellow at UC Berkeley</a>. I received my <a href=publication/social-and-affective-machine-learning/>PhD from MIT</a>, where I worked on <a href=./tag/affective-computing/>Affective Computing</a> and <a href=./tag/deep-learning>deep</a>/<a href=./tag/reinforcement-learning>reinforcement</a>/<a href=./tag/machine-learning>machine learning</a>. My work has received the <a href=publication/interactive-musical-improvisation-with-magenta/>best demo</a> award at NeurIPS 2016, best paper at the NeurIPS workshops on <a href=./publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/>ML for Healthcare</a> and <a href=./publication/learning-social-learning>Cooperative AI</a>, and an <a href=publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/>honourable mention for best paper at ICML 2019</a>; it has also been featured in <a href=https://www.sciencemag.org/news/2021/01/who-needs-teacher-artificial-intelligence-designs-lesson-plans-itself target=_blank rel=noopener>Science Magazine</a>, <a href=https://www.technologyreview.com/s/603003/ai-songsmith-cranks-out-surprisingly-catchy-tunes/ target=_blank rel=noopener>MIT Technology Review</a>, <a href=https://www.nationalgeographic.com/environment/2019/07/artificial-intelligence-climate-change/ target=_blank rel=noopener>National Geographic</a>, <a href=https://spectrum.ieee.org/tech-talk/computing/software/deepmind-teaches-ai-teamwork target=_blank rel=noopener>IEEE Spectrum</a>, <a href=https://qz.com/1209466/google-is-building-ai-to-make-humans-smile/ target=_blank rel=noopener>Quartz</a>, <a href=http://www.bostonmagazine.com/news/blog/2015/01/05/smiletracker-captures-photos-internet/ target=_blank rel=noopener>Boston Magazine</a>, and on <a href=https://www.cbc.ca/news/canada/saskatchewan/regina-woman-develops-smile-app-at-mit-1.2886943 target=_blank rel=noopener>CBC radio</a>. I have interned at DeepMind, Google Brain, and worked as an OpenAI Scholars mentor.</p><p><i class="fas fa-download pr-1 fa-fw"></i><a href=/uploads/cv_natasha_jaques.pdf target=_blank>Download my CV</a>.</p></div><div class=row><div class=col-md-5><div class=section-subheading>Interests</div><ul class="ul-interests mb-0"><li>Reinforcement Learning</li><li>Multi-Agent Learning and Coordination</li><li>Affective Computing</li><li>Machine Learning</li><li>Deep Learning</li></ul></div><div class=col-md-7><div class=section-subheading>Education</div><ul class="ul-edu fa-ul mb-0"><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>PhD in the Media Lab, 2019</p><p class=institution>Massachusetts Institute of Technology</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>MSc in Computer Science, 2014</p><p class=institution>University of British Columbia</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>BSc in Computer Science, 2012</p><p class=institution>University of Regina</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>BA in Psychology, 2012</p><p class=institution>University of Regina</p></div></li></ul></div></div></div></div></div></section><section id=featured class="home-section wg-featured"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Featured Publications</h1></div><div class="col-12 col-lg-8"><div class=card-simple><div class=article-metadata><div><span>Michael Dennis</span><span>*</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>Eugene Vinitsky</span>, <span>Alexandre Bayen</span>, <span>Stuart Russell</span>, <span>Andrew Critch</span>, <span>Sergey Levine</span></div><span class=article-date>2020</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS)</em> <strong>Oral (top 1% of submissions)</strong></span></div><a href=/publication/paired/><img src=/publication/paired/featured_hu0186d5a70d0f2d327cff176d5e7f9ddc_218764_808x455_fit_q90_lanczos_2.png class=article-banner alt="Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design" loading=lazy></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/paired/>Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design</a></div><a href=/publication/paired/ class=summary-link><div class=article-style><p>PAIRED trains an agent to generate environments that maximize regret between a pair of learning agents. This creates feasible yet challenging environments, which exploit weaknesses in the agents to make them more robust. PAIRED significantly improves generalization to novel tasks.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/abs/2012.02096 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/paired/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/google-research/google-research/tree/master/social_rl/adversarial_env target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/bit.ly/paired_poster target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.youtube.com/channel/UCI6dkF8eNrCz6XiBJlV9fmw/videos target=_blank rel=noopener>Videos</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://neurips.cc/virtual/2020/public/poster_985e9a46e10005356bbaf194249f6856.html target=_blank rel=noopener>NeurIPS Oral</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.sciencemag.org/news/2021/01/who-needs-teacher-artificial-intelligence-designs-lesson-plans-itself target=_blank rel=noopener>Science article</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ai.googleblog.com/2021/03/paired-new-multi-agent-approach-for.html target=_blank rel=noopener>Google AI Blog</a></div></div><div class=card-simple><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>J. H. Shen</span><span>*</span>, <span>A. Ghandeharioun</span>, <span>C. Ferguson</span>, <span>A. Lapedriza</span>, <span>N. Jones</span>, <span>S. Gu</span>, <span>R. Picard</span></div><span class=article-date>2020</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Empirical Methods in Natural Language Processing (EMNLP)</em></span></div><a href=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/><img src=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/featured_hud9f1a8371ef1afad2d7e710a56a9cc97_688018_808x455_fit_q90_lanczos_2.png class=article-banner alt="Human-Centric Dialog Training via Offline Reinforcement Learning" loading=lazy></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/>Human-Centric Dialog Training via Offline Reinforcement Learning</a></div><a href=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/ class=summary-link><div class=article-style><p>We train dialog models with interactive data from conversations with real humans, using a novel Offline RL technique based on KL-control. Rather than rely on manual ratings, we learn from implicit signals like sentiment, and show that this results in better performance.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/pdf/2010.05848.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/natashamjaques/neural_chat target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/affect.media.mit.edu/neural_chat/datasets/reddit_casual_preprocessed.tar.gz target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://virtual.2020.emnlp.org/paper_main.2410.html target=_blank rel=noopener>EMNLP Talk</a></div></div><div class=card-simple><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>A. Lazaridou</span>, <span>E. Hughes</span>, <span>C. Gulcehre</span>, <span>P. A. Ortega</span>, <span>D. J. Strouse</span>, <span>J.Z. Leibo</span>, <span>N. de Freitas</span></div><span class=article-date>2019</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference on Machine Learning (ICML)</em> <strong>Best Paper Honourable Mention (top 0.26% of submissions)</strong></span></div><a href=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/><img src=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/featured_huec237f5424ca94f9712aa265373e0f2d_109370_808x455_fit_q90_lanczos_2.png class=article-banner alt="Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning" loading=lazy></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/>Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning</a></div><a href=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/ class=summary-link><div class=article-style><p>Social influence is a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning, through rewarding agents for having causal influence over other agents' actions, thus increasing mutual information between agents' actions. Optimizing for influence leads to agents learning emergent communication protocols. Unlike prior work, influence can be computed in a fully decentralized manner.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/pdf/1810.08647.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.youtube.com/channel/UCNzeAAPyZaX4EDr720q5msg target=_blank rel=noopener>Videos</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.facebook.com/watch/live/?v=355035025132741&ref=watch_permalink" target=_blank rel=noopener>ICML talk</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://spectrum.ieee.org/tech-talk/computing/software/deepmind-teaches-ai-teamwork target=_blank rel=noopener>IEEE Spectrum article</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://medium.com/syncedreview/icml-2019-google-eth-zurich-mpi-is-cambridge-prowler-io-share-best-paper-honours-4aeabd5c9fc8 target=_blank rel=noopener>ICML 2019 Best Papers</a></div></div><div class=card-simple><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>S. Taylor</span><span>*</span>, <span>E. Nosakhare</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2017</span>
<span class=middot-divider></span><span class=pub-publication>In <em>IEEE Transactions on Affective Computing (TAFFC)</em>; <em>NeurIPS Machine Learning for Healthcare (ML4HC) Workshop</em> <strong>Best Paper</strong></span></div><a href=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/><img src=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/featured_huc186f698c5fa4c42a5a6008bd314e0ac_60822_808x455_fit_q90_lanczos_2.png class=article-banner alt="Personalized Multitask Learning for Predicting Tomorrow's Mood, Stress, and Health" loading=lazy></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/>Personalized Multitask Learning for Predicting Tomorrow's Mood, Stress, and Health</a></div><a href=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/ class=summary-link><div class=article-style><p>Traditional, one-size-fits-all machine learning models fail to account for individual differences in predicting wellbeing outcomes like stress, mood, and health. Instead, we personalize models to the individual using multi-task learning (MTL), employing hierarchical Bayes, kernel-based and deep neural network MTL models to improve prediction accuracy by 13-23%.</p></div></a><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/mitmedialab/PersonalizedMultitaskLearning target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://pdfs.semanticscholar.org/b228/7a406985980515d5cc63e9b37fb17c5186f8.pdf target=_blank rel=noopener>ML4HC Best Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/pdfs/17.TaylorJaques-PredictingTomorrowsMoods.pdf target=_blank rel=noopener>TAFFC Journal Paper</a></div></div></div></div></div></section><section id=projects class="home-section wg-portfolio"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Publications</h1></div><div class="col-12 col-lg-8"><p>To find relevant content, try <a href=./publication/>searching publications</a> or filtering using the buttons below. A * denotes equal contribution.</p><span class="d-none default-project-filter">*</span><div class=project-toolbar><div class=project-filters><div class=btn-toolbar><div class="btn-group flex-wrap"><a href=# data-filter=* class="btn btn-primary btn-lg active">All</a>
<a href=# data-filter=.js-id-Affective-Computing class="btn btn-primary btn-lg">Affective Computing</a>
<a href=# data-filter=.js-id-Social-Learning class="btn btn-primary btn-lg">Social Learning</a>
<a href=# data-filter=.js-id-Multi-Agent class="btn btn-primary btn-lg">Multi-Agent</a>
<a href=# data-filter=.js-id-Emergent-Complexity class="btn btn-primary btn-lg">Emergent Complexity</a>
<a href=# data-filter=.js-id-Cooperation class="btn btn-primary btn-lg">Cooperation</a>
<a href=# data-filter=.js-id-Communication-and-Language class="btn btn-primary btn-lg">Communication and Language</a>
<a href=# data-filter=.js-id-Human-AI-Interaction class="btn btn-primary btn-lg">Human-AI Interaction</a>
<a href=# data-filter=.js-id-Generalization class="btn btn-primary btn-lg">Generalization</a></div></div></div></div><div class="isotope projects-container row js-layout-row"><div class="col-12 isotope-item js-id-Emergent-Complexity js-id-Multi-Agent js-id-Intrinsic-Motivation js-id-Reinforcement-Learning js-id-Deep-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/explore-and-control-with-adversarial-surprise/>Explore and Control with Adversarial Surprise</a></div><a href=/publication/explore-and-control-with-adversarial-surprise/ class=summary-link><div class=article-style>Adversarial Surprise creates a competitive game between an Expore policy and a Control policy, which fight to maximize and minimize the amount of entropy an RL agent experiences. We show both theoretically and empirically that this technique fully explores the state space of partially-observed, stochastic environments.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Fickinger</span><span>*</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>S. Parajuli</span>, <span>M. Chang</span>, <span>N. Rhinehart</span>, <span>G. Berseth</span>, <span>S. Russell</span>, <span>S. Levine</span></div><span class=article-date>2021</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS) (submitted)</em></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/explore-and-control-with-adversarial-surprise/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/explore-and-control-with-adversarial-surprise/><img style=width:300px src=/publication/explore-and-control-with-adversarial-surprise/compact_huc6c3ce620201bb8c7060779d4ed52a28_144473_300x0_resize_lanczos_2.png alt="Explore and Control with Adversarial Surprise" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Emergent-Complexity js-id-Multi-Agent js-id-Generalization js-id-Reinforcement-Learning js-id-Deep-Learning js-id-Web-Navigation"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/adversarial-environment-generation-for-learning-to-navigate-the-web/>Adversarial Environment Generation for Learning to Navigate the Web</a></div><a href=/publication/adversarial-environment-generation-for-learning-to-navigate-the-web/ class=summary-link><div class=article-style>We analyze and improve upon PAIRED in the case of learning to generate challenging compositional tasks. We apply our improved algorithm to generating a curriculum of novel websites, in order to train RL agents that can navigate web pages.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>I. Gur</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>K. Malta</span>, <span>M. Tiwari</span>, <span>H. Lee</span>, <span>A. Faust</span></div><span class=article-date>2021</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS) (submitted)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/abs/2103.019917 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/adversarial-environment-generation-for-learning-to-navigate-the-web/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/adversarial-environment-generation-for-learning-to-navigate-the-web/><img style=width:300px src=/publication/adversarial-environment-generation-for-learning-to-navigate-the-web/compact_hu7dc49dc34a48ac2cf1cbe1a059b778c0_44325_300x0_resize_lanczos_2.png alt="Adversarial Environment Generation for Learning to Navigate the Web" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Social-Learning js-id-Multi-Agent js-id-Generalization js-id-Reinforcement-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/learning-social-learning/>Learning Social Learning</a></div><a href=/publication/learning-social-learning/ class=summary-link><div class=article-style>Model-free RL agents fail to learn from experts present in multi-agent environments. By adding a model-based auxiliary loss, we induce social learning, which allows agents to learn how to learn from experts. When deployed to novel environments with new experts, they use social learning to determine how to solve the task, and generalize better than agents trained alone with RL or imitation learning.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>Kamal Ndousse</span>, <span>Douglas Eck</span>, <span>Sergey Levine</span>, <span class=author-highlighted>Natasha Jaques</span></div><span class=article-date>2021</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference on Machine Learning (ICML);</em> <em>NeurIPS Cooperative AI Workshop</em> <strong>Best Paper</strong></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/abs/2010.00581 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/learning-social-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/kandouss/marlgrid target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://slideslive.com/38938232/learning-social-learning?ref=account-folder-62099-folders" target=_blank rel=noopener>Cooperative AI talk</a></div></div><div class=ml-3><a href=/publication/learning-social-learning/><img style=width:300px src=/publication/learning-social-learning/compact_huf868c3ebd1f8795d3075bfbec778af36_146141_300x0_resize_lanczos_2.png alt="Learning Social Learning" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Social-Learning js-id-Multi-Agent js-id-Generalization js-id-Reinforcement-Learning js-id-Successor-Features"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/>PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning</a></div><a href=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/ class=summary-link><div class=article-style>PsiPhi-Learning learns successor representations for the policies of other agents and the ego agent, using a shared underlying state representation. Learning from other agents helps the agent take better actions at inference time, and learning from RL experience improves modeling of other agents.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Filos</span>, <span>C. Lyle</span>, <span>Y. Gal</span>, <span>S. Levine</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>G. Farquhar</span><span>*</span></div><span class=article-date>2021</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference on Machine Learning (ICML)</em> <strong>Long talk (top 3% of submissions)</strong></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/pdf/2102.12560.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/><img style=width:300px src=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/compact_hu7430deb91a31ed263679bc79272f4499_35810_300x0_resize_lanczos_2.png alt="PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Multi-Agent js-id-Cooperation js-id-Intrinsic-Motivation js-id-Social-Learning js-id-Reinforcement-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/joint-attention-for-multiagent-coordination-and-social-learning/>Joint Attention for Multi-Agent Coordination and Social Learning</a></div><a href=/publication/joint-attention-for-multiagent-coordination-and-social-learning/ class=summary-link><div class=article-style>Joint attention is a critical component of human social cognition. In this paper, we ask whether a mechanism based on shared visual attention can be useful for improving multi-agent coordination and social learning.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>D. Lee</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>J. Kew</span>, <span>D. Eck</span>, <span>D. Schuurmans</span>, <span>A. Faust</span></div><span class=article-date>2021</span>
<span class=middot-divider></span><span class=pub-publication>In <em>ICRA Social Intelligence Workshop</em> <strong>Spotlight talk</strong></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/abs/2104.07750 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/joint-attention-for-multiagent-coordination-and-social-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/google-research/google-research/tree/master/social_rl/multiagent_tfagents/joint_attention target=_blank rel=noopener>Code</a></div></div><div class=ml-3><a href=/publication/joint-attention-for-multiagent-coordination-and-social-learning/><img style=width:300px src=/publication/joint-attention-for-multiagent-coordination-and-social-learning/compact_hue339bf41b041e14e5bfbb28aaed1fe45_240472_300x0_resize_lanczos_2.png alt="Joint Attention for Multi-Agent Coordination and Social Learning" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Emergent-Complexity js-id-Multi-Agent js-id-Generalization js-id-Deep-Learning js-id-Reinforcement-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/paired/>Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design</a></div><a href=/publication/paired/ class=summary-link><div class=article-style>PAIRED trains an agent to generate environments that maximize regret between a pair of learning agents. This creates feasible yet challenging environments, which exploit weaknesses in the agents to make them more robust. PAIRED significantly improves generalization to novel tasks.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>Michael Dennis</span><span>*</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>Eugene Vinitsky</span>, <span>Alexandre Bayen</span>, <span>Stuart Russell</span>, <span>Andrew Critch</span>, <span>Sergey Levine</span></div><span class=article-date>2020</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS)</em> <strong>Oral (top 1% of submissions)</strong></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/abs/2012.02096 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/paired/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/google-research/google-research/tree/master/social_rl/adversarial_env target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/bit.ly/paired_poster target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.youtube.com/channel/UCI6dkF8eNrCz6XiBJlV9fmw/videos target=_blank rel=noopener>Videos</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://neurips.cc/virtual/2020/public/poster_985e9a46e10005356bbaf194249f6856.html target=_blank rel=noopener>NeurIPS Oral</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.sciencemag.org/news/2021/01/who-needs-teacher-artificial-intelligence-designs-lesson-plans-itself target=_blank rel=noopener>Science article</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ai.googleblog.com/2021/03/paired-new-multi-agent-approach-for.html target=_blank rel=noopener>Google AI Blog</a></div></div><div class=ml-3><a href=/publication/paired/><img style=width:300px src=/publication/paired/compact_huc914225ca940260e2866390a3c6be412_103258_300x0_resize_lanczos_2.png alt="Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Human-AI-Interaction js-id-Social-Learning js-id-Affective-Computing js-id-Offline-RL js-id-Reinforcement-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/>Human-Centric Dialog Training via Offline Reinforcement Learning</a></div><a href=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/ class=summary-link><div class=article-style>We train dialog models with interactive data from conversations with real humans, using a novel Offline RL technique based on KL-control. Rather than rely on manual ratings, we learn from implicit signals like sentiment, and show that this results in better performance.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>J. H. Shen</span><span>*</span>, <span>A. Ghandeharioun</span>, <span>C. Ferguson</span>, <span>A. Lapedriza</span>, <span>N. Jones</span>, <span>S. Gu</span>, <span>R. Picard</span></div><span class=article-date>2020</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Empirical Methods in Natural Language Processing (EMNLP)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/pdf/2010.05848.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/natashamjaques/neural_chat target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/affect.media.mit.edu/neural_chat/datasets/reddit_casual_preprocessed.tar.gz target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://virtual.2020.emnlp.org/paper_main.2410.html target=_blank rel=noopener>EMNLP Talk</a></div></div><div class=ml-3><a href=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/><img style=width:300px src=/publication/humancentric-dialog-training-via-offline-reinforcement-learning/compact_hucee76efa9adcfddb2507c2302e8a6874_104998_300x0_resize_lanczos_2.png alt="Human-Centric Dialog Training via Offline Reinforcement Learning" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Communication-and-Language js-id-Affective-Computing js-id-Machine-Learning js-id-Human-AI-Interaction js-id-Deep-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/approximating-interactive-human-evaluation-with-selfplay-for-opendomai/>Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems</a></div><a href=/publication/approximating-interactive-human-evaluation-with-selfplay-for-opendomai/ class=summary-link><div class=article-style>Existing metrics for automatically evaluating dialog models correlate poorly with human judgements, and are evaluated on static conversation snippets. Instead, we deploy bots to interact live with humans, then approximate human ratings with state-of-the-art accuracy using conversations generated with self-play.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Ghandeharioun</span><span>*</span>, <span>J. H. Shen</span><span>*</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>C. Ferguson</span>, <span>N. Jones</span>, <span>A. Lapedriza</span>, <span>R. Picard</span></div><span class=article-date>2019</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/abs/1906.09308 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/approximating-interactive-human-evaluation-with-selfplay-for-opendomai/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/natashamjaques/neural_chat target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/affect.media.mit.edu/neural_chat/datasets/reddit_casual_preprocessed.tar.gz target=_blank rel=noopener>Dataset</a></div></div><div class=ml-3><a href=/publication/approximating-interactive-human-evaluation-with-selfplay-for-opendomai/><img style=width:300px src=/publication/approximating-interactive-human-evaluation-with-selfplay-for-opendomai/compact_hu66e60844ef6f4d3eeada98744e8de13e_292165_300x0_resize_lanczos_2.png alt="Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Machine-Learning js-id-Healthcare"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/automatic-triage-and-analysis-of-online-suicide-risk-with-document-emb/>Automatic Triage and Analysis of Online Suicide Risk with Document Embeddings and Latent Dirichlet Allocation</a></div><a href=/publication/automatic-triage-and-analysis-of-online-suicide-risk-with-document-emb/ class=summary-link><div class=article-style>To predict which users are at risk of suicide based on a small dataset of online posts, we leverage pre-trained sentence embeddings from large language models, and achieve high F1 scores (.83-.92). We further analyze users' posts to determine which topics are most associated with suicidal users.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>N. Jones</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>P. Pataranutaporn</span>, <span>A. Ghandeharioun</span>, <span>R. Picard</span></div><span class=article-date>2019</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Affective Computing and Intelligence Interaction (ACII) workshop on Machine Learning for Mental Health</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="/https:/drive.google.com/file/d/1muoFj_BXJUZCRyjCLEX9DxKOz7b9nKtj/view?usp=sharing" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/automatic-triage-and-analysis-of-online-suicide-risk-with-document-emb/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/automatic-triage-and-analysis-of-online-suicide-risk-with-document-emb/><img style=width:300px src=/publication/automatic-triage-and-analysis-of-online-suicide-risk-with-document-emb/compact_huabc2c2a55c60ce3e36a4b80033463568_83028_300x0_resize_lanczos_2.png alt="Automatic Triage and Analysis of Online Suicide Risk with Document Embeddings and Latent Dirichlet Allocation" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Communication-and-Language"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/hierarchical-reinforcement-learning-for-opendomain-dialog/>Hierarchical Reinforcement Learning for Open-Domain Dialog</a></div><a href=/publication/hierarchical-reinforcement-learning-for-opendomain-dialog/ class=summary-link><div class=article-style>For the first time, we use hierarchical reinforcement learning to train open-domain dialog models, enabling the optimization of long-term, conversational, rewards, including reducing the toxicity of generated language. Our approach provides significant improvements over state-of-the-art dialog models.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Saleh</span><span>*</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>A. Ghandeharioun</span>, <span>J. H. Shen</span>, <span>R. Picard</span></div><span class=article-date>2019</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Association for the Advancement of Artificial Intelligence (AAAI)</em> <strong>Oral (top 7.8% of submissions)</strong></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/abs/1909.07547 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/hierarchical-reinforcement-learning-for-opendomain-dialog/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/natashamjaques/neural_chat/tree/master/HierarchicalRL target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/affect.media.mit.edu/neural_chat/datasets/reddit_casual_preprocessed.tar.gz target=_blank rel=noopener>Dataset</a></div></div><div class=ml-3><a href=/publication/hierarchical-reinforcement-learning-for-opendomain-dialog/><img style=width:300px src=/publication/hierarchical-reinforcement-learning-for-opendomain-dialog/compact_huab7f30131204beb0dd6375a7a0eb8990_154201_300x0_resize_lanczos_2.png alt="Hierarchical Reinforcement Learning for Open-Domain Dialog" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Human-AI-Interaction js-id-Multi-Agent js-id-Cooperation js-id-Communication-and-Language js-id-Generalization js-id-Social-Learning js-id-Deep-Learning js-id-Reinforcement-Learning js-id-Machine-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/social-and-affective-machine-learning/>Social and Affective Machine Learning</a></div><a href=/publication/social-and-affective-machine-learning/ class=summary-link><div class=article-style>My PhD Thesis spans both Social Reinforcement Learning and Affective Computing, investigating how affective and social intelligence can enhance machine learning algorithms, and how machine learning can enhance our ability to predict and understand human affective and social phenomena.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span></div><span class=article-date>2019</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Massachusetts Institute of Technology</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/www.media.mit.edu/publications/social-and-affective-machine-learning/ target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/social-and-affective-machine-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.media.mit.edu/events/natasha-jacques-dissertation-defense/ target=_blank rel=noopener>Thesis Defense</a></div></div><div class=ml-3><a href=/publication/social-and-affective-machine-learning/><img style=width:300px src=/publication/social-and-affective-machine-learning/compact_hu92d99a804c27a12d0bbd429fe5c42e10_180638_300x0_resize_lanczos_2.png alt="Social and Affective Machine Learning" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Cooperation js-id-Multi-Agent js-id-Communication-and-Language js-id-Intrinsic-Motivation js-id-Reinforcement-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/>Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning</a></div><a href=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/ class=summary-link><div class=article-style>Social influence is a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning, through rewarding agents for having causal influence over other agents' actions, thus increasing mutual information between agents' actions. Optimizing for influence leads to agents learning emergent communication protocols. Unlike prior work, influence can be computed in a fully decentralized manner.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>A. Lazaridou</span>, <span>E. Hughes</span>, <span>C. Gulcehre</span>, <span>P. A. Ortega</span>, <span>D. J. Strouse</span>, <span>J.Z. Leibo</span>, <span>N. de Freitas</span></div><span class=article-date>2019</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference on Machine Learning (ICML)</em> <strong>Best Paper Honourable Mention (top 0.26% of submissions)</strong></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/pdf/1810.08647.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.youtube.com/channel/UCNzeAAPyZaX4EDr720q5msg target=_blank rel=noopener>Videos</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.facebook.com/watch/live/?v=355035025132741&ref=watch_permalink" target=_blank rel=noopener>ICML talk</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://spectrum.ieee.org/tech-talk/computing/software/deepmind-teaches-ai-teamwork target=_blank rel=noopener>IEEE Spectrum article</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://medium.com/syncedreview/icml-2019-google-eth-zurich-mpi-is-cambridge-prowler-io-share-best-paper-honours-4aeabd5c9fc8 target=_blank rel=noopener>ICML 2019 Best Papers</a></div></div><div class=ml-3><a href=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/><img style=width:300px src=/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/compact_hue1151792439960bc84c366ff900f0c0a_61909_300x0_resize_lanczos_2.png alt="Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Climate-Change js-id-Machine-Learning js-id-Deep-Learning js-id-Reinforcement-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/tackling-climate-change-with-machine-learning/>Tackling Climate Change with Machine Learning</a></div><a href=/publication/tackling-climate-change-with-machine-learning/ class=summary-link><div class=article-style>This paper comprehensively surveys the ways in which machine learning could be usefully deployed in the fight against climate change. From smart grids to disaster management, we identify high impact problems and outline how machine learning can be employed to address them.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>D. Rolnick</span>, <span>P. L. Donti</span>, <span>L. H. Kaack</span>, <span>K. Kochanski</span>, <span>A. Lacoste</span>, <span>K. Sankaran</span>, <span>A. S. Ross</span>, <span>N. Milojevic-Dupont</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>A. Waldman-Brown</span>, <span>A. Luccioni</span>, <span>T. Maharaj</span>, <span>E. D. Sherwin</span>, <span>S. K. Mukkavilli</span>, <span>K. P. Kording</span>, <span>C. Gomes</span>, <span>A. Y. Ng</span>, <span>D. Hassabis</span>, <span>J. C. Platt</span>, <span>F. Creutzig</span>, <span>J. Chayes</span>, <span>Y. Bengio</span></div><span class=article-date>2019</span>
<span class=middot-divider></span><span class=pub-publication>In <em>(Arxiv preprint)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/abs/1906.05433 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/tackling-climate-change-with-machine-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.climatechange.ai/ target=_blank rel=noopener>CCAI Organization</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.technologyreview.com/2019/06/20/134864/ai-climate-change-machine-learning/ target=_blank rel=noopener>MIT Tech Review article</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.nationalgeographic.com/environment/article/artificial-intelligence-climate-change target=_blank rel=noopener>National Geographic article</a></div></div><div class=ml-3><a href=/publication/tackling-climate-change-with-machine-learning/><img style=width:300px src=/publication/tackling-climate-change-with-machine-learning/compact_huef4affcf62397ca1d23e957050e42994_211889_300x0_resize_lanczos_2.png alt="Tackling Climate Change with Machine Learning" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Social-Learning js-id-Human-AI-Interaction js-id-Affective-Computing js-id-Generative-Models js-id-Deep-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/learning-via-social-awareness-improving-a-deep-generative-sketching-mo/>Learning via Social Awareness: Improving a Deep Generative Sketching Model with Facial Feedback</a></div><a href=/publication/learning-via-social-awareness-improving-a-deep-generative-sketching-mo/ class=summary-link><div class=article-style>We show the outputs of a generative model of sketches to human observers and record their facial expressions. Using only a small number of facial expression samples, we are able to tune the model to produce drawings that are significantly better rated by humans.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>J. McCleary</span>, <span>J. Engel</span>, <span>D. Ha</span>, <span>F. Bertsch</span>, <span>D. Eck</span>, <span>R. Picard</span></div><span class=article-date>2018</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference on Learning Representations (ICLR) workshop</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/pdf/1802.04877.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/learning-via-social-awareness-improving-a-deep-generative-sketching-mo/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/learning-via-social-awareness-improving-a-deep-generative-sketching-mo/><img style=width:300px src=/publication/learning-via-social-awareness-improving-a-deep-generative-sketching-mo/compact_hu05080f9b36edf6ab84c6a6644d3e33d8_525540_300x0_resize_lanczos_2.png alt="Learning via Social Awareness: Improving a Deep Generative Sketching Model with Facial Feedback" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Sensors js-id-Physiology"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/vomit-comet-physiology-autonomic-changes-in-novice-flyers/>Vomit Comet Physiology: Autonomic Changes in Novice Flyers</a></div><a href=/publication/vomit-comet-physiology-autonomic-changes-in-novice-flyers/ class=summary-link><div class=article-style>During a zero-gravity parabolic flight, we recorded participants' heart rate, accelerometer, and skin conductance measurements as well as their self-report nausea, anxiety, and excitement. Statistical analysis revealed that skin conductance is predictive of nausea, while heart rate is predictive of anxiety and excitement.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>K. Johnson</span>, <span>S. Taylor</span>, <span>S. Fedor</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>W. Chen</span>, <span>R. Picard</span></div><span class=article-date>2018</span>
<span class=middot-divider></span><span class=pub-publication>In <em>IEEE Engineering in Medicine and Biology Society (EMBC)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="/https:/dspace.mit.edu/bitstream/handle/1721.1/123805/18.Johnson-etal_EMBC18_VomitComet.pdf?sequence=1&isAllowed=y" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/vomit-comet-physiology-autonomic-changes-in-novice-flyers/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/vomit-comet-physiology-autonomic-changes-in-novice-flyers/><img style=width:300px src=/publication/vomit-comet-physiology-autonomic-changes-in-novice-flyers/compact_hubd65089fb19795e91750ea4f9066e2e4_424657_300x0_resize_lanczos_2.png alt="Vomit Comet Physiology: Autonomic Changes in Novice Flyers" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Sensors js-id-Physiology js-id-Healthcare"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/importance-of-sleep-data-in-predicting-nextday-stress-happiness-and-he/>Importance of Sleep Data in Predicting Next-Day Stress, Happiness, and Health in College Students</a></div><a href=/publication/importance-of-sleep-data-in-predicting-nextday-stress-happiness-and-he/ class=summary-link><div class=article-style>We train personalized hierarchical Bayes models to predict individual&rsquo;s next-day stress, happiness, and health, and examine the effect of including features related to sleep in the model. Including sleep features significantly improves performance when predicting happiness.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>S. Taylor</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>Sano, A. E. Nosakhare</span>, <span>E. B. Klerman</span>, <span>R. Picard</span></div><span class=article-date>2017</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Journal of Sleep and Sleep Disorders Research (suppl_1)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/affect.media.mit.edu/pdfs/17.Taylor-etal-MoodPrediction-SLEEP2017-Poster.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/importance-of-sleep-data-in-predicting-nextday-stress-happiness-and-he/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/importance-of-sleep-data-in-predicting-nextday-stress-happiness-and-he/><img style=width:300px src=/publication/importance-of-sleep-data-in-predicting-nextday-stress-happiness-and-he/compact_hu69dd9bb9b0627592c4cc65fd4599288a_569771_300x0_resize_lanczos_2.png alt="Importance of Sleep Data in Predicting Next-Day Stress, Happiness, and Health in College Students" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Generalization js-id-Healthcare"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/multimodal-autoencoder-a-deep-learning-approach-to-filling-in-missing-/>Multimodal Autoencoder: A Deep Learning Approach to Filling in Missing Sensor Data and Enabling Better Mood Prediction</a></div><a href=/publication/multimodal-autoencoder-a-deep-learning-approach-to-filling-in-missing-/ class=summary-link><div class=article-style>Predicting signals like stress and health depends on collecting noisy data from a number of modalities, e.g. smartphone data, or physiological data from a wrist-worn sensor. Our method can continue making accurate predictions even when a modality goes missing; for example, if the person forgets to wear their sensor.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>S. Taylor</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2017</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference on Affective Computing and Intelligent Interaction (ACII)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/affect.media.mit.edu/pdfs/17.Jaques_autoencoder_ACII.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/multimodal-autoencoder-a-deep-learning-approach-to-filling-in-missing/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/natashamjaques/MultimodalAutoencoder target=_blank rel=noopener>Code</a></div></div><div class=ml-3><a href=/publication/multimodal-autoencoder-a-deep-learning-approach-to-filling-in-missing-/><img style=width:300px src=/publication/multimodal-autoencoder-a-deep-learning-approach-to-filling-in-missing/compact_hu3fe3ad9fe6ef5b7415513152a12330fa_280332_300x0_resize_lanczos_2.png alt="Multimodal Autoencoder: A Deep Learning Approach to Filling in Missing Sensor Data and Enabling Better Mood Prediction" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Generalization js-id-Healthcare js-id-Wellbeing js-id-Multi-task-Learning js-id-Machine-Learning js-id-Deep-Learning js-id-Hierarchical-Bayes js-id-Kernel-Methods"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/>Personalized Multitask Learning for Predicting Tomorrow's Mood, Stress, and Health</a></div><a href=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/ class=summary-link><div class=article-style>Traditional, one-size-fits-all machine learning models fail to account for individual differences in predicting wellbeing outcomes like stress, mood, and health. Instead, we personalize models to the individual using multi-task learning (MTL), employing hierarchical Bayes, kernel-based and deep neural network MTL models to improve prediction accuracy by 13-23%.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>S. Taylor</span><span>*</span>, <span>E. Nosakhare</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2017</span>
<span class=middot-divider></span><span class=pub-publication>In <em>IEEE Transactions on Affective Computing (TAFFC)</em>; <em>NeurIPS Machine Learning for Healthcare (ML4HC) Workshop</em> <strong>Best Paper</strong></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/mitmedialab/PersonalizedMultitaskLearning target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://pdfs.semanticscholar.org/b228/7a406985980515d5cc63e9b37fb17c5186f8.pdf target=_blank rel=noopener>ML4HC Best Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://affect.media.mit.edu/pdfs/17.TaylorJaques-PredictingTomorrowsMoods.pdf target=_blank rel=noopener>TAFFC Journal Paper</a></div></div><div class=ml-3><a href=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/><img style=width:300px src=/publication/personalized-multitask-learning-for-predicting-tomorrows-mood-stress-a/compact_hud33baf28447e2e2dbecfa802753044ed_198069_300x0_resize_lanczos_2.png alt="Personalized Multitask Learning for Predicting Tomorrow's Mood, Stress, and Health" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Generalization js-id-Healthcare js-id-Multi-task-Learning js-id-Machine-Learning js-id-Deep-Learning js-id-Gaussian-Processes"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/predicting-tomorrows-mood-health-and-stress-level-using-personalized-m/>Predicting Tomorrows Mood, Health, and Stress Level using Personalized Multitask Learning and Domain Adaptation</a></div><a href=/publication/predicting-tomorrows-mood-health-and-stress-level-using-personalized-m/ class=summary-link><div class=article-style>Modeling measures like mood, stress, and health using a monolithic machine learning model leads to low prediction accuracy. Instead, we develop personalized regression models using multi-task learning and Gaussian Processes, leading to dramatic improvements in next-day predictions.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>O. Rudovic</span>, <span>S. Taylor</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2017</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Proceedings of Machine Learning Research</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/http:/proceedings.mlr.press/v66/jaques17a/jaques17a.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/predicting-tomorrows-mood-health-and-stress-level-using-personalized-m/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/predicting-tomorrows-mood-health-and-stress-level-using-personalized-m/><img style=width:300px src=/publication/predicting-tomorrows-mood-health-and-stress-level-using-personalized-m/compact_hu07571ff498d15a20626e0024e5f9b59d_190694_300x0_resize_lanczos_2.png alt="Predicting Tomorrows Mood, Health, and Stress Level using Personalized Multitask Learning and Domain Adaptation" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Sequence-Modeling js-id-Communication-and-Language js-id-Music-Generation js-id-Drug-Discovery js-id-Healthcare js-id-Generalization js-id-Transfer-Learning js-id-KL-control js-id-Reinforcement-Learning js-id-Machine-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/sequence-tutor-conservative-finetuning-of-sequence-generation-models-w/>Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control</a></div><a href=/publication/sequence-tutor-conservative-finetuning-of-sequence-generation-models-w/ class=summary-link><div class=article-style>To combine supervised learning on data with reinforcement learning, we pre-train a supervised data prior, and penalize KL-divergence from this model using RL training. This enables effective learning of complex sequence-modeling problems for which we wish to match the data while optimizing external metrics like drug effectiveness. The approach produces compelling results in the disparate domains of music generation and drug discovery.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>S. Gu</span>, <span>D. Bahdanau</span>, <span>J. M. Hernandez-Lobato</span>, <span>R. E. Turner</span>, <span>D. Eck</span></div><span class=article-date>2017</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference on Machine Learning (ICML)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/arxiv.org/abs/1611.02796 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/sequence-tutor-conservative-finetuning-of-sequence-generation-models-w/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/tensorflow/magenta/tree/master/magenta/models/rl_tuner target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://vimeo.com/240608475 target=_blank rel=noopener>ICML talk</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/abBfZB5DlSY target=_blank rel=noopener>Generated music</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://magenta.tensorflow.org/2016/11/09/tuning-recurrent-networks-with-reinforcement-learning target=_blank rel=noopener>Magenta blog</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.technologyreview.com/2016/11/30/155729/ai-songsmith-cranks-out-surprisingly-catchy-tunes/ target=_blank rel=noopener>MIT Tech Review article</a></div></div><div class=ml-3><a href=/publication/sequence-tutor-conservative-finetuning-of-sequence-generation-models-w/><img style=width:300px src=/publication/sequence-tutor-conservative-finetuning-of-sequence-generation-models-w/compact_hu516b72a52b0e20f8f6988d67cd0bf946_71842_300x0_resize_lanczos_2.png alt="Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Behavior-Change js-id-Wellbeing"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/bitxbit-encouraging-behavior-change-with-n2-experiments/>BITxBIT: Encouraging Behavior Change with N=2 Experiments</a></div><a href=/publication/bitxbit-encouraging-behavior-change-with-n2-experiments/ class=summary-link><div class=article-style>To help promote behavior change, we leverage the power of social obligation, and conduct an experiment in which participants are paired together and asked to design a Behavioral Intervention Technology (BIT) customized to suit their partners behavior change goal.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>T. Rich</span>, <span>K. Dinakar</span>, <span>N. Farve</span>, <span>W.V. Chen</span>, <span>P. Maes</span>, <span>R. Picard</span></div><span class=article-date>2016</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Proceedings of the CHI Conference Extended Abstracts on Human Factors</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/http:/affect.media.mit.edu/pdfs/16.Jaques-etal-CHI.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/bitxbit-encouraging-behavior-change-with-n2-experiments/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/bitxbit-encouraging-behavior-change-with-n2-experiments/><img style=width:300px src=/publication/bitxbit-encouraging-behavior-change-with-n2-experiments/compact_hu689cef82a8ea011d5247054f1d69a63e_91538_300x0_resize_lanczos_2.png alt="BITxBIT: Encouraging Behavior Change with N=2 Experiments" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Music-Generation js-id-Demo js-id-Deep-Learning js-id-Generative-Models js-id-Sequence-Modeling"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/interactive-musical-improvisation-with-magenta/>Interactive Musical Improvisation with Magenta</a></div><a href=/publication/interactive-musical-improvisation-with-magenta/ class=summary-link><div class=article-style>This demo deployed RL Tuner and other Magenta music generation models into an interactive interface in which users can collaborate creatively with a machine learning model. The interface supports call and response interaction, automatically generating an accompaniment to the user&rsquo;s melody, or melody morphing: responding both with variations on the user&rsquo;s melody and a bass accompaniment.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Roberts</span>, <span>J. Engel</span>, <span>C. Hawthorne</span>, <span>I. Simon</span>, <span>E. Waite</span>, <span>S. Oore</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>C. Resnick</span>, <span>D. Eck</span></div><span class=article-date>2016</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS)</em> <strong>Best Demo</strong></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/interactive-musical-improvisation-with-magenta/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/magenta/magenta target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/https:/www.youtube.com/watch?v=QlVoR1jQrPk&t=1s" target=_blank rel=noopener>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://nips.cc/Conferences/2016/Schedule?showEvent=6307" target=_blank rel=noopener>NeurIPS Demo</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://magenta.tensorflow.org/ target=_blank rel=noopener>Magenta</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://magenta.tensorflow.org/2016/12/16/nips-demo target=_blank rel=noopener>Blog post</a></div></div><div class=ml-3><a href=/publication/interactive-musical-improvisation-with-magenta/><img style=width:300px src=/publication/interactive-musical-improvisation-with-magenta/compact_hue937982e008a1df44b297e8de8d4e977_857119_300x0_resize_lanczos_2.png alt="Interactive Musical Improvisation with Magenta" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Wellbeing js-id-Healthcare js-id-Physiology js-id-Machine-Learning js-id-Kernel-Methods js-id-Multi-task-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/machine-learning-of-sleep-and-wake-behaviors-to-classify-selfreported-/>Machine Learning of Sleep and Wake Behaviors to Classify Self-Reported Evening Mood</a></div><a href=/publication/machine-learning-of-sleep-and-wake-behaviors-to-classify-selfreported-/ class=summary-link><div class=article-style>Machine learning applied to nightly data from sensors and smartphones, shows value for predicting college students mood the following evening. Using multi-task learning to simultaneously predicted related wellbeing factors like health, energy, stress, and alertness improves performance.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>S. Taylor</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>A. Sano</span>, <span>A. Azaria</span>, <span>A. Ghandeharioun</span>, <span>R. Picard</span></div><span class=article-date>2016</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Sleep</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/affect.media.mit.edu/pdfs/16.Taylor-ClassifyingSelfReportedMood-SLEEP2016.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/machine-learning-of-sleep-and-wake-behaviors-to-classify-selfreported/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/mitmedialab/PersonalizedMultitaskLearning target=_blank rel=noopener>Code</a></div></div><div class=ml-3><a href=/publication/machine-learning-of-sleep-and-wake-behaviors-to-classify-selfreported-/><img style=width:300px src=/publication/machine-learning-of-sleep-and-wake-behaviors-to-classify-selfreported/compact_hu67a9313843bd4d0757512aa17e4e037d_288870_300x0_resize_lanczos_2.png alt="Machine Learning of Sleep and Wake Behaviors to Classify Self-Reported Evening Mood" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Communication-and-Language js-id-Affective-Computing js-id-Human-AI-Interaction"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/personality-attitudes-and-bonding-in-conversations/>Personality, Attitudes, and Bonding in Conversations</a></div><a href=/publication/personality-attitudes-and-bonding-in-conversations/ class=summary-link><div class=article-style>We collect observational data from real human conversations, and develop a measure of how much participants experienced bonding or chemistry. We analyze the effects of personality and attitudes on bonding, and find that attentiveness and excitement are more effective at promoting bonding than traits like attractiveness and humour.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>Y. K. Kim</span>, <span>\& Picard R. Picard R</span></div><span class=article-date>2016</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Intelligent Virtual Agents (IVA)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/http:/affect.media.mit.edu/pdfs/16.Jaques-IVApersonality.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/personality-attitudes-and-bonding-in-conversations/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/personality-attitudes-and-bonding-in-conversations/><img style=width:300px src=/publication/personality-attitudes-and-bonding-in-conversations/compact_hu6e52db2b195bd435c65d794401961075_82784_300x0_resize_lanczos_2.png alt="Personality, Attitudes, and Bonding in Conversations" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Music-Generation js-id-Generalization"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/>Tuning Recurrent Neural Networks with Reinforcement Learning</a></div><a href=/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/ class=summary-link><div class=article-style>Generating music using traditional supervised sequence models suffers from known failure modes, including the inability to produce coherent global structure. Music is an interesting sequence generation problem, because musical compositions adhere to known rules. We impose these rules with a novel algorithm combining RL and supervised learning.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>S. Gu</span>, <span>R. E. Turner</span>, <span>D. Eck</span></div><span class=article-date>2016</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference on Learning Representations (ICLR) - workshop</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="/https:/openreview.net/pdf?id=Syyv2e-Kx" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/tensorflow/magenta/tree/master/magenta/models/rl_tuner target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://magenta.tensorflow.org/2016/11/09/tuning-recurrent-networks-with-reinforcement-learning target=_blank rel=noopener>Magenta blog</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.technologyreview.com/2016/11/30/155729/ai-songsmith-cranks-out-surprisingly-catchy-tunes/ target=_blank rel=noopener>MIT Tech Review article</a></div></div><div class=ml-3><a href=/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/><img style=width:300px src=/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/compact_hu5c8d57118dda98d2151abb98b5a96367_50040_300x0_resize_lanczos_2.png alt="Tuning Recurrent Neural Networks with Reinforcement Learning" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Human-AI-Interaction js-id-Intelligent-Virtual-Agents js-id-Machine-Learning js-id-Deep-Learning js-id-Communication-and-Language"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/understanding-and-predicting-bonding-in-conversations-using-thin-slice/>Understanding and Predicting Bonding in Conversations Using Thin Slices of Facial Expressions and Body Language</a></div><a href=/publication/understanding-and-predicting-bonding-in-conversations-using-thin-slice/ class=summary-link><div class=article-style>Given only one-minute slices of facial expressions and body language, we use machine learning to accurately predict whether two humans having a conversation will bond with each other. We analyze factors which lead to bonding and discover that synchrony in body language and appropriate, empathetic facial expressions lead to higher bonding.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>D. McDuff</span>, <span>Y. K. Kim</span>, <span>\& Picard R. Picard R</span></div><span class=article-date>2016</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Intelligent Virtual Agents (IVA)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/http:/affect.media.mit.edu/pdfs/16.Jaques-IVAbonding.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/understanding-and-predicting-bonding-in-conversations-using-thin-slice/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/understanding-and-predicting-bonding-in-conversations-using-thin-slice/><img style=width:300px src=/publication/understanding-and-predicting-bonding-in-conversations-using-thin-slice/compact_hu695472130ff302a84610d3bb9619ffff_126124_300x0_resize_lanczos_2.png alt="Understanding and Predicting Bonding in Conversations Using Thin Slices of Facial Expressions and Body Language" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Active-Learning js-id-Physiology js-id-Affective-Computing js-id-Sensors js-id-Machine-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/active-learning-for-electrodermal-activity-classification/>Active learning for Electrodermal Activity classification</a></div><a href=/publication/active-learning-for-electrodermal-activity-classification/ class=summary-link><div class=article-style>We use labels provided by domain experts to classify whether artifacts are present in an Electrodermal Activity signal. Through the use of active learning, we improve sample efficiency and reduce the burden on human experts by as much as 84%, while offering the same or improved performance.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>V. Xia</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>S. Taylor</span>, <span>S. Fedor</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span><span class=pub-publication>In <em>IEEE Conference on Signal Processing in Medicine and Biology (SPMB)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/dspace.mit.edu/openaccess-disseminate/1721.1/109392 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/active-learning-for-electrodermal-activity-classification/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/active-learning-for-electrodermal-activity-classification/><img style=width:300px src=/publication/active-learning-for-electrodermal-activity-classification/compact_hu5d816a23bf8e0beebb3ea391d5419449_46952_300x0_resize_lanczos_2.png alt="Active learning for Electrodermal Activity classification" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Physiology js-id-Sensors js-id-Machine-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/automatic-identification-of-artifacts-in-electrodermal-activity-data/>Automatic identification of artifacts in Electrodermal Activity data</a></div><a href=/publication/automatic-identification-of-artifacts-in-electrodermal-activity-data/ class=summary-link><div class=article-style>Ambulatory measurement of Electrodermal Activity (EDA) from the wrist has important clinical benefits, such as predicting mood, stress, health, or even seizures. However, ambulatory measurement is noisy, and artifacts can easily be mistaken for true Skin Conductance Responses (SCRs). In addition to our paper which describes a machine learning method for detecting artifacts with 95% test accuracy, we built EDA Explorer, an open-source tool that allows users to automatically detect artifacts and SCRs within their data.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>S. Taylor</span><span>*</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>W. Chen</span>, <span>S. Fedor</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/www.ncbi.nlm.nih.gov/pmc/articles/PMC5413200/ target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/automatic-identification-of-artifacts-in-electrodermal-activity-data/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/MITMediaLabAffectiveComputing/eda-explorer target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://eda-explorer.media.mit.edu/ target=_blank rel=noopener>EDA Explorer tool</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=s_QqG-QtMdM" target=_blank rel=noopener>Artifact detection tutorial</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=BbnOmQzxBh4" target=_blank rel=noopener>SCR detection tutorial</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://eda-explorer.media.mit.edu/research/ target=_blank rel=noopener>Research which uses EDA Explorer</a></div></div><div class=ml-3><a href=/publication/automatic-identification-of-artifacts-in-electrodermal-activity-data/><img style=width:300px src=/publication/automatic-identification-of-artifacts-in-electrodermal-activity-data/compact_hu7294be0f4dfdda5d2236b827ed850b64_89306_300x0_resize_lanczos_2.png alt="Automatic identification of artifacts in Electrodermal Activity data" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Behavior-Change js-id-Wellbeing js-id-Affective-Computing"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/engaging-the-workplace-with-challenges/>Engaging the workplace with challenges</a></div><a href=/publication/engaging-the-workplace-with-challenges/ class=summary-link><div class=article-style>The Challenge is a tool aimed at promoting social connections and decreasing sedentary activity in a workplace environment. Participants are paired with a partner to complete short physical challenges, leveraging social obligation and social consensus to drive behavior change.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>N. Farve</span></div><span class=article-date>2015</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference on Persuasive Technologies</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="/https:/drive.google.com/file/d/168_D8T70JbQsrhTU0_6ZuFUyD14NbY1-/view?usp=sharing" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/engaging-the-workplace-with-challenges/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/https:/www.youtube.com/watch?v=coyW2yzQhFg" target=_blank rel=noopener>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/15Pimm1FwwxSPPX04KTr84ilVHJvVExPd/view?usp=sharing" target=_blank rel=noopener>Extended paper</a></div></div><div class=ml-3><a href=/publication/engaging-the-workplace-with-challenges/><img style=width:300px src=/publication/engaging-the-workplace-with-challenges/compact_hu10f79fef9b503b31f453628fb0c5ea45_416386_300x0_resize_lanczos_2.png alt="Engaging the workplace with challenges" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Generalization js-id-Affective-Computing js-id-Wellbeing js-id-Healthcare js-id-Physiology js-id-Multi-task-Learning js-id-Machine-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/multitask-multikernel-learning-for-estimating-individual-wellbeing/>Multi-task Multi-Kernel Learning for Estimating Individual Wellbeing</a></div><a href=/publication/multitask-multikernel-learning-for-estimating-individual-wellbeing/ class=summary-link><div class=article-style>Wellbeing is a complex internal state consisting of several related dimensions, such as happiness, stress, energy, and health. We use Multi-task Multi-kernel learning to classify them simultaneously, leading to significant performance approvements.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>S. Taylor</span><span>*</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Neural Information Processing Systems (NeurIPS) Workshop on Multimodal Machine Learning</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/affect.media.mit.edu/pdfs/15.Jaques-etal-NIPSMMML.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/multitask-multikernel-learning-for-estimating-individual-wellbeing/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/github.com/mitmedialab/PersonalizedMultitaskLearning/tree/master/MTMKL target=_blank rel=noopener>Code</a></div></div><div class=ml-3><a href=/publication/multitask-multikernel-learning-for-estimating-individual-wellbeing/><img style=width:300px src=/publication/multitask-multikernel-learning-for-estimating-individual-wellbeing/compact_hu239a8902a06eac83286a213ce87b3050_96885_300x0_resize_lanczos_2.png alt="Multi-task Multi-Kernel Learning for Estimating Individual Wellbeing" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Healthcare js-id-Physiology js-id-Sensors"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/predicting-students-happiness-from-physiology-phone-mobility-and-behav/>Predicting students' happiness from physiology, phone, mobility, and behavioral data</a></div><a href=/publication/predicting-students-happiness-from-physiology-phone-mobility-and-behav/ class=summary-link><div class=article-style>We train machine learning models to predict students' happiness from extensive data comprising physiological signals, location, smartphone logs, and behavioral questions. Analyzing which features provide the highest information gain reveals that skin conductance during sleep, social interaction, exercise, and fewer phone screen hours are all positively associated with happiness.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>S. Taylor</span><span>*</span>, <span>A. Azaria</span>, <span>A. Ghandeharioun</span>, <span>A. Sano</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference on Affective Computing and Intelligent Interaction (ACII)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/dam-prod.media.mit.edu/x/files/pdfs/15.Jaques-Taylor-et-al-PredictingHappiness.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/predicting-students-happiness-from-physiology-phone-mobility-and-behav/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5431070/ target=_blank rel=noopener>NCBI link</a></div></div><div class=ml-3><a href=/publication/predicting-students-happiness-from-physiology-phone-mobility-and-behav/><img style=width:300px src=/publication/predicting-students-happiness-from-physiology-phone-mobility-and-behav/compact_hu1fc4810ff4bc072cc6b18f280751529d_106282_300x0_resize_lanczos_2.png alt="Predicting students' happiness from physiology, phone, mobility, and behavioral data" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Healthcare js-id-Physiology js-id-Sensors"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/prediction-of-happysad-mood-from-daily-behaviors-and-previous-sleep-hi/>Prediction of happy-sad mood from daily behaviors and previous sleep history</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Sano</span>, <span>A.Z. Yu</span>, <span>A.W. McHill</span>, <span>A.J. Phillips</span>, <span>S. Taylor</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>C. A. Czeisler</span>, <span>E. B. Klerman</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/www.ncbi.nlm.nih.gov/pmc/articles/PMC4768795/ target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/prediction-of-happysad-mood-from-daily-behaviors-and-previous-sleep-hi/cite.bib>Cite</a></div></div><div class=ml-3></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Healthcare js-id-Physiology js-id-Sensors"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/recognizing-academic-performance-sleep-quality-stress-level-and-mental/>Recognizing academic performance, sleep quality, stress level, and mental health using personality traits, wearable sensors and mobile phones</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span>A. Sano</span>, <span>A. J. Phillips</span>, <span>A. Z. Yu</span>, <span>A. W. McHill</span>, <span>S. Taylor</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>C. A. Czeisler</span>, <span>E. B. Klerman</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Wearable and Implantable Body Sensor Networks (BSN)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/www.ncbi.nlm.nih.gov/pmc/articles/PMC5431072/ target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/recognizing-academic-performance-sleep-quality-stress-level-and-mental/cite.bib>Cite</a></div></div><div class=ml-3></div></div></div><div class="col-12 isotope-item js-id-Human-AI-Interaction js-id-Affective-Computing js-id-Wellbeing"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/smiletracker-automatically-and-unobtrusively-recording-smiles-and-thei/>SmileTracker: Automatically and Unobtrusively Recording Smiles and their Context.</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>W. V. Chen</span>, <span>R. Picard</span></div><span class=article-date>2015</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Proceedings of the CHI Conference Extended Abstracts</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/affect.media.mit.edu/pdfs/15.jaques-chen-picard-CHI.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/smiletracker-automatically-and-unobtrusively-recording-smiles-and-thei/cite.bib>Cite</a></div></div><div class=ml-3></div></div></div><div class="col-12 isotope-item js-id-Physiology js-id-Sensors js-id-Affective-Computing js-id-Healthcare"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/waveletbased-motion-artifact-removal-for-electrodermal-activity/>Wavelet-based motion artifact removal for Electrodermal Activity</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span>W. Chen</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>S. Taylor</span>, <span>A. Sano</span>, <span>S. Fedor</span>, <span>\& Picard R. Picard R</span></div><span class=article-date>2015</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/www.ncbi.nlm.nih.gov/pmc/articles/PMC5413204/ target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/waveletbased-motion-artifact-removal-for-electrodermal-activity/cite.bib>Cite</a></div></div><div class=ml-3></div></div></div><div class="col-12 isotope-item js-id-Intelligent-Tutoring-Systems js-id-Human-AI-Interaction js-id-Affective-Computing"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/predicting-affect-from-gaze-data-during-interaction-with-an-intelligen/>Predicting Affect from Gaze Data During Interaction with an Intelligent Tutoring System</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>C. Conati</span>, <span>J. M. Harley</span>, <span>R. Azevedo</span></div><span class=article-date>2014</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Intelligent Tutoring Systems</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/http:/www.cs.ubc.ca/~conati/my-papers/ITS-Natasha-2014.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/predicting-affect-from-gaze-data-during-interaction-with-an-intelligen/cite.bib>Cite</a></div></div><div class=ml-3></div></div></div><div class="col-12 isotope-item js-id-Affective-Computing js-id-Intelligent-Tutoring-Systems js-id-Human-AI-Interaction"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/predicting-affect-in-an-intelligent-tutoring-system/>Predicting Affect in an Intelligent Tutoring System</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span></div><span class=article-date>2014</span>
<span class=middot-divider></span><span class=pub-publication>In <em>University of British Columbia</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/open.library.ubc.ca/collections/ubctheses/24/items/1.0135541 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/predicting-affect-in-an-intelligent-tutoring-system/cite.bib>Cite</a></div></div><div class=ml-3></div></div></div><div class="col-12 isotope-item js-id-Gesture-Recognition js-id-Deep-Learning js-id-Machine-Learning"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/a-comparison-of-random-forests-and-dropout-nets-for-sign-language-reco/>A Comparison of Random Forests and Dropout Nets for Sign Language Recognition with the Kinect</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>J. Nutini</span></div><span class=article-date>2013</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Unpublished manuscript</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/http:/www.cs.ubc.ca/~jaquesn/MachineLearningProject.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/a-comparison-of-random-forests-and-dropout-nets-for-sign-language-reco/cite.bib>Cite</a></div></div><div class=ml-3></div></div></div><div class="col-12 isotope-item js-id-Intelligent-Tutoring-Systems js-id-Human-AI-Interaction js-id-Affective-Computing"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/emotionally-adaptive-intelligent-tutoring-systems-using-pomdps/>Emotionally Adaptive Intelligent Tutoring Systems using POMDPs</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span></div><span class=article-date>2013</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Unpublished manuscript</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/http:/www.cs.ubc.ca/~jaquesn/POMDPPaper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/emotionally-adaptive-intelligent-tutoring-systems-using-pomdps/cite.bib>Cite</a></div></div><div class=ml-3></div></div></div><div class="col-12 isotope-item js-id-Machine-Learning js-id-Compression"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/fast-johnsonlindenstrauss-transform-for-classification-of-high-dimensi/>Fast JohnsonLindenstrauss transform for classification of high dimensional data</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span></div><span class=article-date>2013</span>
<span class=middot-divider></span><span class=pub-publication>In <em>Unpublished manuscript</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/http:/www.cs.ubc.ca/~jaquesn/MachineLearningTheory.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/fast-johnsonlindenstrauss-transform-for-classification-of-high-dimensi/cite.bib>Cite</a></div></div><div class=ml-3></div></div></div><div class="col-12 isotope-item js-id-Intelligent-Tutoring-Systems js-id-Human-AI-Interaction"><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/understanding-attention-to-adaptive-hints-in-educational-games-an-eyet/>Understanding attention to adaptive hints in educational games: an eye-tracking study</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span>C. Conati</span>, <span class=author-highlighted>Natasha Jaques</span>, <span>M. Muir</span></div><span class=article-date>2013</span>
<span class=middot-divider></span><span class=pub-publication>In <em>International Journal of Artificial Intelligence in Education</em></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/https:/link.springer.com/article/10.1007/s40593-013-0002-8 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/understanding-attention-to-adaptive-hints-in-educational-games-an-eyet/cite.bib>Cite</a></div></div><div class=ml-3></div></div></div></div></div></div></div></section><section id=tags class="home-section wg-tag-cloud"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Popular Topics</h1></div><div class="col-12 col-lg-8"><div class=tag-cloud><a href=/tag/affective-computing/ style=font-size:1.9811493369140072rem>Affective Computing</a>
<a href=/tag/behavior-change/ style=font-size:.7rem>Behavior Change</a>
<a href=/tag/communication-and-language/ style=font-size:1.3257341657562653rem>Communication and Language</a>
<a href=/tag/cooperation/ style=font-size:.9025230450977161rem>Cooperation</a>
<a href=/tag/deep-learning/ style=font-size:1.5949539098045675rem>Deep Learning</a>
<a href=/tag/emergent-complexity/ style=font-size:.9025230450977161rem>Emergent Complexity</a>
<a href=/tag/generalization/ style=font-size:1.5514931811901975rem>Generalization</a>
<a href=/tag/healthcare/ style=font-size:1.5949539098045675rem>Healthcare</a>
<a href=/tag/human-ai-interaction/ style=font-size:1.5514931811901975rem>Human-AI Interaction</a>
<a href=/tag/intelligent-tutoring-systems/ style=font-size:1.0462154323534256rem>Intelligent Tutoring Systems</a>
<a href=/tag/intrinsic-motivation/ style=font-size:.9025230450977161rem>Intrinsic Motivation</a>
<a href=/tag/machine-learning/ style=font-size:1.6719495981096912rem>Machine Learning</a>
<a href=/tag/multi-agent/ style=font-size:1.3924308647068513rem>Multi-Agent</a>
<a href=/tag/multi-task-learning/ style=font-size:1.0462154323534256rem>Multi-task Learning</a>
<a href=/tag/music-generation/ style=font-size:.9025230450977161rem>Music Generation</a>
<a href=/tag/physiology/ style=font-size:1.5038873392649943rem>Physiology</a>
<a href=/tag/reinforcement-learning/ style=font-size:1.5514931811901975rem>Reinforcement Learning</a>
<a href=/tag/sensors/ style=font-size:1.3924308647068513rem>Sensors</a>
<a href=/tag/social-learning/ style=font-size:1.2487384774511419rem>Social Learning</a>
<a href=/tag/wellbeing/ style=font-size:1.2487384774511419rem>Wellbeing</a></div></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Built using <a href=https://gohugo.io target=_blank rel=noopener>Hugo</a> and <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a>.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/en/js/wowchemy.min.4bba0826db6409c865d2e7b99039d6d0.js></script><script async defer src=https://buttons.github.io/buttons.js></script></body></html>