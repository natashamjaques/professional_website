<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.2.0 for Hugo"><meta name=author content="Natasha Jaques"><meta name=description content="PsiPhi-Learning learns successor representations for the policies of other agents and the ego agent, using a shared underlying state representation. Learning from other agents helps the agent take better actions at inference time, and learning from RL experience improves modeling of other agents."><link rel=alternate hreflang=en-us href=https://natashamjaques.github.io/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.08f2e04360a1c87f5ad39547c02bf219.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/media/icon_hua64261c49917fe48d46061d344453b21_144873_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hua64261c49917fe48d46061d344453b21_144873_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://natashamjaques.github.io/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Natasha Jaques"><meta property="og:url" content="https://natashamjaques.github.io/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/"><meta property="og:title" content="PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning | Natasha Jaques"><meta property="og:description" content="PsiPhi-Learning learns successor representations for the policies of other agents and the ego agent, using a shared underlying state representation. Learning from other agents helps the agent take better actions at inference time, and learning from RL experience improves modeling of other agents."><meta property="og:image" content="https://natashamjaques.github.io/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/featured.png"><meta property="twitter:image" content="https://natashamjaques.github.io/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2021-05-08T00:00:00+00:00"><meta property="article:modified_time" content="2021-07-19T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://natashamjaques.github.io/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/"},"headline":"PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning","image":["https://natashamjaques.github.io/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/featured.png"],"datePublished":"2021-05-08T00:00:00Z","dateModified":"2021-07-19T00:00:00Z","author":{"@type":"Person","name":"A. Filos"},"publisher":{"@type":"Organization","name":"Natasha Jaques","logo":{"@type":"ImageObject","url":"https://natashamjaques.github.io/media/icon_hua64261c49917fe48d46061d344453b21_144873_192x192_fill_lanczos_center_3.png"}},"description":"PsiPhi-Learning learns successor representations for the policies of other agents and the ego agent, using a shared underlying state representation. Learning from other agents helps the agent take better actions at inference time, and learning from RL experience improves modeling of other agents."}</script><title>PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning | Natasha Jaques</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=f96a69baf451d6ee7ac851c19facae42><script src=/js/wowchemy-init.min.4be02a3b391999348b0c7478778a0e4b.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Natasha Jaques</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Natasha Jaques</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#awards><span>Awards</span></a></li><li class=nav-item><a class=nav-link href=/#press><span>Press</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Featured</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#tags><span>Topics</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#communities><span>Communities</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://twitter.com/natashajaques data-toggle=tooltip data-placement=bottom title=Twitter target=_blank rel=noopener aria-label=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning</h1><div class=article-metadata><div><span>A. Filos</span>, <span>C. Lyle</span>, <span>Y. Gal</span>, <span>S. Levine</span>, <span class=author-highlighted>Natasha Jaques</span><span>*</span>, <span>G. Farquhar</span><span>*</span></div><span class=article-date>2021</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=https://arxiv.org/pdf/2102.12560.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header" href=https://sites.google.com/view/psiphi-learning target=_blank rel=noopener>Project</a>
<a class="btn btn-outline-primary btn-page-header" href="https://www.youtube.com/watch?v=8aHElAZB5uo" target=_blank rel=noopener>ICML talk</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:306px><div style=position:relative><img src=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/featured_hub7aef73d31182521e37d39b3a512f1e0_318949_720x0_resize_lanczos_3.png alt class=featured-image></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>We study reinforcement learning (RL) with no-reward demonstrations, a setting in which an RL agent has access to additional data from the interaction of other agents with the same environment. However, it has no access to the rewards or goals of these agents, and their objectives and levels of expertise may vary widely. These assumptions are common in multi-agent settings, such as autonomous driving. To effectively use this data, we turn to the framework of successor features. This allows us to disentangle shared features and dynamics of the environment from agent-specific rewards and policies. We propose a multi-task inverse reinforcement learning (IRL) algorithm, called \emph{inverse temporal difference learning} (ITD), that learns shared state features, alongside per-agent successor features and preference vectors, purely from demonstrations without reward labels. We further show how to seamlessly integrate ITD with learning from online environment interactions, arriving at a novel algorithm for reinforcement learning with demonstrations, called ΨΦ-learning (pronounced `Sci-Fi&rsquo;). We provide empirical evidence for the effectiveness of ΨΦ-learning as a method for improving RL, IRL, imitation, and few-shot transfer, and derive worst-case bounds for its performance in zero-shot transfer to new tasks.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">In <em>International Conference on Machine Learning (ICML)</em> <strong>Oral (top 3% of submissions)</strong></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tag/social-learning/>Social Learning</a>
<a class="badge badge-light" href=/tag/multi-agent/>Multi-Agent</a>
<a class="badge badge-light" href=/tag/generalization/>Generalization</a>
<a class="badge badge-light" href=/tag/reinforcement-learning/>Reinforcement Learning</a>
<a class="badge badge-light" href=/tag/successor-features/>Successor Features</a>
<a class="badge badge-light" href=/tag/deep-learning/>Deep Learning</a></div><div class="media author-card content-widget-hr"><a href=https://natashamjaques.github.io/><img class="avatar mr-3 avatar-circle" src=/author/natasha-jaques/avatar_hu535661bce2c742bf7a0f6a055ac0b6d1_1827718_270x270_fill_q75_lanczos_center.jpg alt="Natasha Jaques"></a><div class=media-body><h5 class=card-title><a href=https://natashamjaques.github.io/>Natasha Jaques</a></h5><p class=card-text>My research is focused on Social Reinforcement Learning&ndash;developing algorithms that use insights from social learning to improve AI agents&rsquo; learning, generalization, coordination, and human-AI interaction.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:natashamjaques@gmail.com><i class="fas fa-envelope"></i></a></li><li><a href=https://twitter.com/natashajaques target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?hl=en&user=8iCb2TwAAAAJ" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/natashamjaques target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=/uploads/cv_natasha_jaques.pdf><i class="fas fa-download"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/publication/learning-social-learning/>Emergent Social Learning via Multi-agent Reinforcement Learning</a></li><li><a href=/publication/social-and-affective-machine-learning/>Social and Affective Machine Learning</a></li><li><a href=/publication/environment-generation-for-zero-shot-compositional-reinforcement-learn/>Environment Generation for Zero-Shot Compositional Reinforcement Learning</a></li><li><a href=/publication/joint-attention-for-multiagent-coordination-and-social-learning/>Joint Attention for Multi-Agent Coordination and Social Learning</a></li><li><a href=/publication/paired/>Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design</a></li></ul></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Built using <a href=https://gohugo.io target=_blank rel=noopener>Hugo</a> and the <a href=https://github.com/wowchemy/starter-hugo-academic target=_blank rel=noopener>Wowchemy academic template</a>. View <a href=https://github.com/natashamjaques/professional_website target=_blank rel=noopener>source</a>.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script>
<script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script>
<script src=/en/js/wowchemy.min.26bc5a5b73c468c9e767656a378ac5e3.js></script>
<script async defer src=https://buttons.github.io/buttons.js></script></body></html>