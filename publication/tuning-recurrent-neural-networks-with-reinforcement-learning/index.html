<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.2.0 for Hugo"><meta name=author content="Natasha Jaques"><meta name=description content="Generating music using traditional supervised sequence models suffers from known failure modes, including the inability to produce coherent global structure. Music is an interesting sequence generation problem, because musical compositions adhere to known rules. We impose these rules with a novel algorithm combining RL and supervised learning."><link rel=alternate hreflang=en-us href=https://natashamjaques.github.io/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.08f2e04360a1c87f5ad39547c02bf219.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/media/icon_hua64261c49917fe48d46061d344453b21_144873_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hua64261c49917fe48d46061d344453b21_144873_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://natashamjaques.github.io/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Natasha Jaques"><meta property="og:url" content="https://natashamjaques.github.io/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/"><meta property="og:title" content="Tuning Recurrent Neural Networks with Reinforcement Learning | Natasha Jaques"><meta property="og:description" content="Generating music using traditional supervised sequence models suffers from known failure modes, including the inability to produce coherent global structure. Music is an interesting sequence generation problem, because musical compositions adhere to known rules. We impose these rules with a novel algorithm combining RL and supervised learning."><meta property="og:image" content="https://natashamjaques.github.io/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/featured.png"><meta property="twitter:image" content="https://natashamjaques.github.io/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2016-01-01T00:00:00+00:00"><meta property="article:modified_time" content="2016-01-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://natashamjaques.github.io/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/"},"headline":"Tuning Recurrent Neural Networks with Reinforcement Learning","image":["https://natashamjaques.github.io/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/featured.png"],"datePublished":"2016-01-01T00:00:00Z","dateModified":"2016-01-01T00:00:00Z","author":{"@type":"Person","name":"Natasha Jaques"},"publisher":{"@type":"Organization","name":"Natasha Jaques","logo":{"@type":"ImageObject","url":"https://natashamjaques.github.io/media/icon_hua64261c49917fe48d46061d344453b21_144873_192x192_fill_lanczos_center_3.png"}},"description":"Generating music using traditional supervised sequence models suffers from known failure modes, including the inability to produce coherent global structure. Music is an interesting sequence generation problem, because musical compositions adhere to known rules. We impose these rules with a novel algorithm combining RL and supervised learning."}</script><title>Tuning Recurrent Neural Networks with Reinforcement Learning | Natasha Jaques</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=25862849c18069f91a4e4b8ba335d08a><script src=/js/wowchemy-init.min.4be02a3b391999348b0c7478778a0e4b.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Natasha Jaques</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Natasha Jaques</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#awards><span>Awards</span></a></li><li class=nav-item><a class=nav-link href=/#press><span>Press</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Featured</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#tags><span>Topics</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#communities><span>Communities</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://twitter.com/natashajaques data-toggle=tooltip data-placement=bottom title=Twitter target=_blank rel=noopener aria-label=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>Tuning Recurrent Neural Networks with Reinforcement Learning</h1><div class=article-metadata><div><span class=author-highlighted>Natasha Jaques</span>, <span>S. Gu</span>, <span>R. E. Turner</span>, <span>D. Eck</span></div><span class=article-date>2016</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href="https://openreview.net/pdf?id=Syyv2e-Kx" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header" href=https://github.com/tensorflow/magenta/tree/master/magenta/models/rl_tuner target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header" href=https://magenta.tensorflow.org/2016/11/09/tuning-recurrent-networks-with-reinforcement-learning target=_blank rel=noopener>Magenta blog</a>
<a class="btn btn-outline-primary btn-page-header" href=https://www.technologyreview.com/2016/11/30/155729/ai-songsmith-cranks-out-surprisingly-catchy-tunes/ target=_blank rel=noopener>MIT Tech Review article</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:204px><div style=position:relative><img src=/publication/tuning-recurrent-neural-networks-with-reinforcement-learning/featured_hu6eec534c775241d62eca57eae409a37b_112661_720x0_resize_lanczos_3.png alt class=featured-image></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>The approach of training sequence models using supervised learning and next-step prediction suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note-RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">In <em>International Conference on Learning Representations (ICLR) - workshop</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tag/music-generation/>Music Generation</a>
<a class="badge badge-light" href=/tag/generalization/>Generalization</a>
<a class="badge badge-light" href=/tag/generative-models/>Generative Models</a>
<a class="badge badge-light" href=/tag/reinforcement-learning/>Reinforcement Learning</a>
<a class="badge badge-light" href=/tag/deep-learning/>Deep Learning</a>
<a class="badge badge-light" href=/tag/sequence-modeling/>Sequence Modeling</a></div><div class="media author-card content-widget-hr"><a href=https://natashamjaques.github.io/><img class="avatar mr-3 avatar-circle" src=/author/natasha-jaques/avatar_hu535661bce2c742bf7a0f6a055ac0b6d1_1827718_270x270_fill_q75_lanczos_center.jpg alt="Natasha Jaques"></a><div class=media-body><h5 class=card-title><a href=https://natashamjaques.github.io/>Natasha Jaques</a></h5><p class=card-text>My research is focused on Social Reinforcement Learning&ndash;developing algorithms that use insights from social learning to improve AI agents&rsquo; learning, generalization, coordination, and human-AI interaction.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:natashamjaques@gmail.com><i class="fas fa-envelope"></i></a></li><li><a href=https://twitter.com/natashajaques target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?hl=en&user=8iCb2TwAAAAJ" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/natashamjaques target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=/uploads/cv_natasha_jaques.pdf><i class="fas fa-download"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/publication/sequence-tutor-conservative-finetuning-of-sequence-generation-models-w/>Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control</a></li><li><a href=/publication/interactive-musical-improvisation-with-magenta/>Interactive Musical Improvisation with Magenta</a></li><li><a href=/publication/environment-generation-for-zero-shot-compositional-reinforcement-learn/>Environment Generation for Zero-Shot Compositional Reinforcement Learning</a></li><li><a href=/publication/learning-social-learning/>Learning Social Learning</a></li><li><a href=/publication/psiphilearning-reinforcement-learning-with-demonstrations-using-succes/>PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning</a></li></ul></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Built using <a href=https://gohugo.io target=_blank rel=noopener>Hugo</a> and the <a href=https://github.com/wowchemy/starter-hugo-academic target=_blank rel=noopener>Wowchemy academic template</a>. View <a href=https://github.com/natashamjaques/professional_website target=_blank rel=noopener>source</a>.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script>
<script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script>
<script src=/en/js/wowchemy.min.26bc5a5b73c468c9e767656a378ac5e3.js></script>
<script async defer src=https://buttons.github.io/buttons.js></script></body></html>